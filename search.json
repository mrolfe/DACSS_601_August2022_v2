[
  {
    "objectID": "posts/challenge8_instructions.html",
    "href": "posts/challenge8_instructions.html",
    "title": "Challenge 8 Instructions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge8_instructions.html#challenge-overview",
    "href": "posts/challenge8_instructions.html#challenge-overview",
    "title": "Challenge 8 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in multiple data sets, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nmutate variables as needed (including sanity checks)\njoin two or more data sets and analyze some aspect of the joined data\n\n(be sure to only include the category tags for the data you use!)"
  },
  {
    "objectID": "posts/challenge8_instructions.html#read-in-data",
    "href": "posts/challenge8_instructions.html#read-in-data",
    "title": "Challenge 8 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nmilitary marriages ⭐⭐\nfaostat ⭐⭐\nrailroads ⭐⭐⭐\nfed_rate ⭐⭐⭐\ndebt ⭐⭐⭐\nus_hh ⭐⭐⭐⭐\nsnl ⭐⭐⭐⭐⭐\n\n\n\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge8_instructions.html#tidy-data-as-needed",
    "href": "posts/challenge8_instructions.html#tidy-data-as-needed",
    "title": "Challenge 8 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\n\n\n\nAre there any variables that require mutation to be usable in your analysis stream? For example, do you need to calculate new values in order to graph them? Can string values be represented numerically? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here."
  },
  {
    "objectID": "posts/challenge8_instructions.html#join-data",
    "href": "posts/challenge8_instructions.html#join-data",
    "title": "Challenge 8 Instructions",
    "section": "Join Data",
    "text": "Join Data\nBe sure to include a sanity check, and double-check that case count is correct!"
  },
  {
    "objectID": "posts/challenge5_solutions.html",
    "href": "posts/challenge5_solutions.html",
    "title": "Challenge 5 Solutions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge5_solutions.html#challenge-overview",
    "href": "posts/challenge5_solutions.html#challenge-overview",
    "title": "Challenge 5 Solutions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread, clean, and tidy data and then…\ncreate at least two univariate visualizations\n\n\ntry to make them “publication” ready\nExplain why you choose the specific graph type\n\n\nCreate at least one bivariate visualization\n\n\ntry to make them “publication” ready\nExplain why you choose the specific graph type\n\nThere is even an R Graph Gallery book to use that summarizes information from the website!\n\ncereal ⭐pathogen cost ⭐Australian Marriage ⭐⭐AB_NYC_2019.csv ⭐⭐⭐Railroads ⭐⭐⭐Public School Characteristics ⭐⭐⭐⭐USA Households ⭐⭐⭐⭐⭐\n\n\nThe cereal dataset includes sodium and sugar content for 20 popular cereals, along with an indicator of cereal category (A, B, or C) but we are not sure what that variable corresponds to.\n\ncereal<-read_csv(\"_data/cereal.csv\")\n\n\nUnivariate Visualizations\nI am interested in the distribution of sodium and sugar content in cereals, lets start by checking out a simple histogram - binned into approximately 25 mg ranges. I do this by setting bins equal to max minur min of the variable, or 14 bins.\n\nggplot(cereal, aes(x=Sodium)) +\n  geom_histogram(bins=14)\n\n\n\n\nIt looks like there are some outliers while most cereals are more clumped together between 100 and 200 mg. Unfortunately, we can’t automatically label outliers, but there is a commonly used trick to add in labels that I can never get to work for a single boxplot. So, I use it for grouped data in the example below, but am cheating by using the car package to label the outliers for the single boxplot - maybe one of you can find a better way!\n\ncar::Boxplot(cereal$Sodium, \n        data=cereal, \n        id=list(labels=cereal$Cereal),\n        cex=0.2)\n\n\n\n\n[1] \"Frosted Mini Wheats\" \"Raisin Bran\"        \n\nis_outlier <- function(x) {\n  return(x < quantile(x, 0.25) - 1.5 * IQR(x) | x > quantile(x, 0.75) + 1.5 * IQR(x))\n}\n\ncereal %>%\n  mutate(Sugar = Sugar * 15) %>%\n  pivot_longer(cols=c(Sodium, Sugar),\n               names_to = \"content\",\n               values_to = \"value\")%>%\n  group_by(content)%>%\n  mutate(outlier = if_else(is_outlier(value), Cereal, NA_character_)) %>%\n  ggplot(., aes(x = content, y = value, color=factor(content))) +\n  geom_boxplot(outlier.shape = NA) +\n  theme(legend.position = \"none\") +\n  geom_text(aes(label = outlier), na.rm = TRUE, show.legend = FALSE) +\n  scale_y_continuous(\"Milligrams (Sodium)\", \n    sec.axis = sec_axis(~ . /15, name = \"Milligrams (Sugar)\")\n  )\n\n\n\n\nHow about sugar? We can set the number of bins to cover 2 grams of sugar, or 9.\n\nggplot(cereal, aes(x=Sugar)) +\n  geom_histogram(bins=9)\n\n\n\n\nIt looks like cereals are more closely grouped with respect to sugar content - and a boxplot indicates no true outliers.\n\nggplot(cereal, aes(y = Sugar)) +\n    geom_boxplot()\n\n\n\n\n\n\nBivariate Visualization(s)\nAre cereals high in sodium low in sugar, or vice versa? To answer this question, lets check out a scatterplot.\n\nggplot(cereal, aes(y=Sugar, x=Sodium)) +\n  geom_point()\n\n\n\n\nIt doesn’t look like there is a systematic relationship. However, this might be different if we added in the types A and C. Also, Raisin Bran seems to be high in both!\n\n\n\nThis dataset includes the total number of cases and total estimated cost for the top 15 pathogens in 2018.\n\npathogen<-readxl::read_excel(\n  \"_data/Total_cost_for_top_15_pathogens_2018.xlsx\",\n  skip=5, \n  n_max=16, \n  col_names = c(\"pathogens\", \"Cases\", \"Cost\"))\n\npathogen\n\n\n\n  \n\n\n\n\nUnivariate Visualizations\nLets check out the distribution of cost and number of cases. There are only 15 observations - even fewer than the number of cereals, and the data are highly skewed. Will the same sorts of visualizations work?\n\nggplot(pathogen, aes(x=Cases)) +\n  geom_histogram()\nggplot(pathogen, aes(x=Cases)) +\n  geom_histogram()+\n  scale_x_continuous(trans = \"log10\")\nggplot(pathogen, aes(x=Cases)) +\n  geom_boxplot()\nggplot(pathogen, aes(x=Cases)) +\n  geom_boxplot()+\n  scale_x_continuous(trans = \"log10\")\n\n\n\n\n\n\nHistogram of Cases\n\n\n\n\n\n\n\nHistogram of (Logged) Cases\n\n\n\n\n\n\n\n\n\nBoxplot of Cases\n\n\n\n\n\n\n\nBoxplot of (Logged) Cases\n\n\n\n\n\n\nThe histogram isn’t ideal, we can see the single outlier - but it is hard to get a grasp on the number of cases of pathogens with lower case counts. Perhaps if we rescaled the number of cases to a log or some other scaling function. As we see below, the logging of the x axis is much more revealing.\nWhat happens when we graph costs?\n\nggplot(pathogen, aes(x=Cost)) +\n  geom_histogram()\nggplot(pathogen, aes(x=Cost)) +\n  geom_histogram()+\n  scale_x_continuous(trans = \"log10\")\n\n\n\n\n\n\nHistogram of Cost\n\n\n\n\n\n\n\nHistogram of (Logged) Cost\n\n\n\n\n\n\n\n\nBivariate Visualization(s)\nGiven what we saw above, lets try a logged and unlogged scatterplot for Cases vs Costs.\n\nggplot(pathogen, aes(x=Cases, y=Cost, label=pathogens)) +\n  geom_point() +\n  scale_x_continuous(labels = scales::comma)+\n  geom_text()\nggplot(pathogen, aes(x=Cases, y=Cost, label=pathogens)) +\n  geom_point()+\n  scale_x_continuous(trans = \"log10\", labels = scales::comma)+\n  scale_y_continuous(trans = \"log10\", labels = scales::comma)+\n  ggrepel::geom_label_repel()\n\n\n\n\n\n\nRelationship between Cases and Total Cost of Pathogens\n\n\n\n\n\n\n\nLogged Relationship between Cases and Total Cost\n\n\n\n\n\n\n\n\n\nIn 2017, Australia conducted a postal survey to gauge citizens’ opinions towards same sex marriage: “Should the law be changed to allow same-sex couples to marry?” The table provided by the Australian Bureau of Statistics includes estimates of the proportion of citizens choosing to 1) vote yes, 2) vote no, 3) vote in an unclear way, or 4) fail to vote. These results are aggregated by Federal Electoral District, which are nested within one of 8 overarching Electoral Divisions. See Challenge 3 for more details.\n\nvote_orig <- readxl::read_excel(\"_data/australian_marriage_law_postal_survey_2017_-_response_final.xls\",\n           sheet=\"Table 2\",\n           skip=7,\n           col_names = c(\"District\", \"Yes\", \"del\", \"No\", rep(\"del\", 6), \"Illegible\", \"del\", \"No Response\", rep(\"del\", 3)))%>%\n  select(!starts_with(\"del\"))%>%\n  drop_na(District)%>%\n  filter(!str_detect(District, \"(Total)\"))%>%\n  filter(!str_starts(District, \"\\\\(\"))\n\nvote<- vote_orig%>%\n  mutate(Division = case_when(\n    str_ends(District, \"Divisions\") ~ District,\n    TRUE ~ NA_character_ ))%>%\n  fill(Division, .direction = \"down\")\nvote<- filter(vote,!str_detect(District, \"Division|Australia\"))\n\nvote_long <- vote%>%\n  pivot_longer(\n    cols = Yes:`No Response`,\n    names_to = \"Response\",\n    values_to = \"Count\"\n  )\n\n\nUnivariate Visualization(s)\nI think I will start out by graphing the overall proportion of Australian citizens who voted yes, no, etc. That requires me to recreate the proportions information we discarded when we read in the data!\n\nvote_long%>%\n  group_by(Response)%>%\n  summarise(Count = sum(Count))%>%\n  ggplot(., aes(x=Response, y=Count))+\n  geom_bar(stat=\"identity\")\n\n\n\n\nHm, I see a few issues. I would like to reorder the Yes and No folks (who voted) and clearly distinguish them from No Response. Plus maybe label the bars with the % vote (or total numbers?) and the axis with the other value.\n\nvote_long%>%\n  mutate(Response = as_factor(Response),\n         Response = fct_relevel(Response, \"Yes\", \"No\", \"Illegible\"))%>%\n  group_by(Response)%>%\n  summarise(Count = sum(Count))%>%\n  ungroup()%>%\n  mutate(perc = Count/sum(Count))%>%\n  ggplot(., aes(y=perc, x=Response))+\n  geom_bar(stat=\"Identity\", alpha=.75) +\n  scale_y_continuous(name= \"Percent of Citizens\", \n                     label = scales::percent) +\n  geom_text(aes(label = Count), size=3, vjust=-.5)\n\n\n\n\n\n\nBivariate Visualization(s)\n\n\n\nThis is a new data set from air bnb, lets check it out.\n\nairb<-read_csv(\"_data/AB_NYC_2019.csv\")\nprint(summarytools::dfSummary(airb,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\nData Frame Summary\nairb\nDimensions: 48895 x 16\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      id\n[numeric]\n      Mean (sd) : 19017143 (10983108)min ≤ med ≤ max:2539 ≤ 19677284 ≤ 36487245IQR (CV) : 19680234 (0.6)\n      48895 distinct values\n      \n      0\n(0.0%)\n    \n    \n      name\n[character]\n      1. Hillside Hotel2. Home away from home3. New york Multi-unit build4. Brooklyn Apartment5. Loft Suite @ The Box Hous6. Private Room7. Artsy Private BR in Fort 8. Private room9. Beautiful Brooklyn Browns10. Cozy Brooklyn Apartment[ 47884 others ]\n      18(0.0%)17(0.0%)16(0.0%)12(0.0%)11(0.0%)11(0.0%)10(0.0%)10(0.0%)8(0.0%)8(0.0%)48758(99.8%)\n      \n      16\n(0.0%)\n    \n    \n      host_id\n[numeric]\n      Mean (sd) : 67620011 (78610967)min ≤ med ≤ max:2438 ≤ 30793816 ≤ 274321313IQR (CV) : 99612390 (1.2)\n      37457 distinct values\n      \n      0\n(0.0%)\n    \n    \n      host_name\n[character]\n      1. Michael2. David3. Sonder (NYC)4. John5. Alex6. Blueground7. Sarah8. Daniel9. Jessica10. Maria[ 11442 others ]\n      417(0.9%)403(0.8%)327(0.7%)294(0.6%)279(0.6%)232(0.5%)227(0.5%)226(0.5%)205(0.4%)204(0.4%)46060(94.2%)\n      \n      21\n(0.0%)\n    \n    \n      neighbourhood_group\n[character]\n      1. Bronx2. Brooklyn3. Manhattan4. Queens5. Staten Island\n      1091(2.2%)20104(41.1%)21661(44.3%)5666(11.6%)373(0.8%)\n      \n      0\n(0.0%)\n    \n    \n      neighbourhood\n[character]\n      1. Williamsburg2. Bedford-Stuyvesant3. Harlem4. Bushwick5. Upper West Side6. Hell's Kitchen7. East Village8. Upper East Side9. Crown Heights10. Midtown[ 211 others ]\n      3920(8.0%)3714(7.6%)2658(5.4%)2465(5.0%)1971(4.0%)1958(4.0%)1853(3.8%)1798(3.7%)1564(3.2%)1545(3.2%)25449(52.0%)\n      \n      0\n(0.0%)\n    \n    \n      latitude\n[numeric]\n      Mean (sd) : 40.7 (0.1)min ≤ med ≤ max:40.5 ≤ 40.7 ≤ 40.9IQR (CV) : 0.1 (0)\n      19048 distinct values\n      \n      0\n(0.0%)\n    \n    \n      longitude\n[numeric]\n      Mean (sd) : -74 (0)min ≤ med ≤ max:-74.2 ≤ -74 ≤ -73.7IQR (CV) : 0 (0)\n      14718 distinct values\n      \n      0\n(0.0%)\n    \n    \n      room_type\n[character]\n      1. Entire home/apt2. Private room3. Shared room\n      25409(52.0%)22326(45.7%)1160(2.4%)\n      \n      0\n(0.0%)\n    \n    \n      price\n[numeric]\n      Mean (sd) : 152.7 (240.2)min ≤ med ≤ max:0 ≤ 106 ≤ 10000IQR (CV) : 106 (1.6)\n      674 distinct values\n      \n      0\n(0.0%)\n    \n    \n      minimum_nights\n[numeric]\n      Mean (sd) : 7 (20.5)min ≤ med ≤ max:1 ≤ 3 ≤ 1250IQR (CV) : 4 (2.9)\n      109 distinct values\n      \n      0\n(0.0%)\n    \n    \n      number_of_reviews\n[numeric]\n      Mean (sd) : 23.3 (44.6)min ≤ med ≤ max:0 ≤ 5 ≤ 629IQR (CV) : 23 (1.9)\n      394 distinct values\n      \n      0\n(0.0%)\n    \n    \n      last_review\n[Date]\n      min : 2011-03-28med : 2019-05-19max : 2019-07-08range : 8y 3m 10d\n      1764 distinct values\n      \n      10052\n(20.6%)\n    \n    \n      reviews_per_month\n[numeric]\n      Mean (sd) : 1.4 (1.7)min ≤ med ≤ max:0 ≤ 0.7 ≤ 58.5IQR (CV) : 1.8 (1.2)\n      937 distinct values\n      \n      10052\n(20.6%)\n    \n    \n      calculated_host_listings_count\n[numeric]\n      Mean (sd) : 7.1 (33)min ≤ med ≤ max:1 ≤ 1 ≤ 327IQR (CV) : 1 (4.6)\n      47 distinct values\n      \n      0\n(0.0%)\n    \n    \n      availability_365\n[numeric]\n      Mean (sd) : 112.8 (131.6)min ≤ med ≤ max:0 ≤ 45 ≤ 365IQR (CV) : 227 (1.2)\n      366 distinct values\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-23\n\n\n\n\nUnivariate Visualizations\n\n\nBivariate Visualization(s)\n\n\n\nThe railroad data contain 2931 county-level aggregated counts of the number of railroad employees in 2012. Counties are embedded within States, and all 50 states plus Canada, overseas addresses in Asia and Europe, and Washington, DC are represented. See challenges 1 and 2 for more information.\n\nrailroad<-readxl::read_excel(\"_data/StateCounty2012.xls\",\n                     skip = 4,\n                     col_names= c(\"state\", \"delete\",  \"county\",\n                                  \"delete\", \"employees\"))%>%\n  select(!contains(\"delete\"))%>%\n  filter(!str_detect(state, \"Total\"))\n\nrailroad<-head(railroad, -2)%>%\n  mutate(county = ifelse(state==\"CANADA\", \"CANADA\", county))\n\nLets create some numerical variables that we can visualize!\n\nrailroad<- railroad%>%\n  group_by(state)%>%\n  mutate(state_employees = sum(employees),\n         state_countries = n_distinct(county))\n\n\nUnivariate Visualizations\n\n\nBivariate Visualization(s)\n\n\n\nThis is another new dataset.\n\nschools<-read_csv(\"_data/Public_School_Characteristics_2017-18.csv\")\nprint(summarytools::dfSummary(schools,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\nData Frame Summary\nschools\nDimensions: 100729 x 79\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      X\n[numeric]\n      Mean (sd) : -92.9 (16.9)min ≤ med ≤ max:-176.6 ≤ -89.3 ≤ 144.9IQR (CV) : 20.2 (-0.2)\n      97136 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Y\n[numeric]\n      Mean (sd) : 37.8 (5.8)min ≤ med ≤ max:-14.3 ≤ 38.8 ≤ 71.3IQR (CV) : 7.7 (0.2)\n      97136 distinct values\n      \n      0\n(0.0%)\n    \n    \n      OBJECTID\n[numeric]\n      Mean (sd) : 50365 (29078.1)min ≤ med ≤ max:1 ≤ 50365 ≤ 100729IQR (CV) : 50364 (0.6)\n      100729 distinct values\n      \n      0\n(0.0%)\n    \n    \n      NCESSCH\n[character]\n      1. 0100005008702. 0100005008713. 0100005008794. 0100005008895. 0100005016166. 0100005021507. 0100006001938. 0100006008729. 01000060087610. 010000600877[ 100719 others ]\n      1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)100719(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      NMCNTY\n[character]\n      1. Los Angeles County2. Cook County3. Maricopa County4. Harris County5. Orange County6. Jefferson County7. Montgomery County8. Washington County9. Wayne County10. Dallas County[ 1949 others ]\n      2264(2.2%)1388(1.4%)1256(1.2%)1142(1.1%)1074(1.1%)980(1.0%)888(0.9%)848(0.8%)817(0.8%)814(0.8%)89258(88.6%)\n      \n      0\n(0.0%)\n    \n    \n      SURVYEAR\n[character]\n      1. 2017-2018\n      100729(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      STABR\n[character]\n      1. CA2. TX3. NY4. FL5. IL6. MI7. OH8. PA9. NC10. NJ[ 46 others ]\n      10323(10.2%)9320(9.3%)4808(4.8%)4375(4.3%)4245(4.2%)3734(3.7%)3610(3.6%)2990(3.0%)2691(2.7%)2595(2.6%)52038(51.7%)\n      \n      0\n(0.0%)\n    \n    \n      LEAID\n[character]\n      1. 72000302. 06227103. 17099304. 12003905. 32000606. 12001807. 12008708. 15000309. 482364010. 1201500[ 17451 others ]\n      1121(1.1%)1009(1.0%)655(0.7%)537(0.5%)381(0.4%)336(0.3%)320(0.3%)294(0.3%)284(0.3%)268(0.3%)95524(94.8%)\n      \n      0\n(0.0%)\n    \n    \n      ST_LEAID\n[character]\n      1. PR-012. CA-19647333. IL-15-016-2990-254. FL-135. NV-026. FL-067. FL-298. HI-0019. TX-10191210. FL-50[ 17451 others ]\n      1121(1.1%)1009(1.0%)655(0.7%)537(0.5%)381(0.4%)336(0.3%)320(0.3%)294(0.3%)284(0.3%)268(0.3%)95524(94.8%)\n      \n      0\n(0.0%)\n    \n    \n      LEA_NAME\n[character]\n      1. PUERTO RICO DEPARTMENT OF2. Los Angeles Unified3. City of Chicago SD 2994. DADE5. CLARK COUNTY SCHOOL DISTR6. BROWARD7. HILLSBOROUGH8. Hawaii Department of Educ9. HOUSTON ISD10. PALM BEACH[ 17147 others ]\n      1121(1.1%)1009(1.0%)655(0.7%)537(0.5%)381(0.4%)336(0.3%)320(0.3%)294(0.3%)284(0.3%)268(0.3%)95524(94.8%)\n      \n      0\n(0.0%)\n    \n    \n      SCH_NAME\n[character]\n      1. Lincoln Elementary School2. Lincoln Elementary3. Jefferson Elementary4. Washington Elementary5. Washington Elementary Sch6. Central Elementary School7. Jefferson Elementary Scho8. Lincoln Elem School9. Central High School10. Roosevelt Elementary[ 88366 others ]\n      64(0.1%)61(0.1%)53(0.1%)49(0.0%)46(0.0%)42(0.0%)33(0.0%)33(0.0%)32(0.0%)32(0.0%)100284(99.6%)\n      \n      0\n(0.0%)\n    \n    \n      LSTREET1\n[character]\n      1. 6420 E. Broadway Blvd. Su2. Box DOE3. 2405 FAIRVIEW SCHOOL RD4. 1820 XENIUM LN N5. Main St6. 335 ALTERNATIVE LN7. 2101 N TWYMAN RD8. 720 9TH AVE9. 50 Moreland Rd.10. 951 W Snowflake Blvd[ 92384 others ]\n      33(0.0%)28(0.0%)22(0.0%)19(0.0%)13(0.0%)12(0.0%)11(0.0%)11(0.0%)10(0.0%)10(0.0%)100560(99.8%)\n      \n      0\n(0.0%)\n    \n    \n      LSTREET2\n[character]\n      1. Suite B2. Ste. 1003. P.O. Box 14974. Suite A5. Suite 2006. Building B7. Ste. 1028. Ste. A9. Suite 110. SUITE 111 HART[ 482 others ]\n      8(1.4%)7(1.2%)6(1.0%)6(1.0%)5(0.8%)4(0.7%)4(0.7%)4(0.7%)4(0.7%)4(0.7%)540(91.2%)\n      \n      100137\n(99.4%)\n    \n    \n      LSTREET3\n[logical]\n      All NA's\n      \n      \n      100729\n(100.0%)\n    \n    \n      LCITY\n[character]\n      1. HOUSTON2. Chicago3. Los Angeles4. BROOKLYN5. SAN ANTONIO6. Phoenix7. BRONX8. DALLAS9. NEW YORK10. Tucson[ 14624 others ]\n      783(0.8%)664(0.7%)577(0.6%)569(0.6%)520(0.5%)446(0.4%)441(0.4%)378(0.4%)359(0.4%)330(0.3%)95662(95.0%)\n      \n      0\n(0.0%)\n    \n    \n      LSTATE\n[character]\n      1. CA2. TX3. NY4. FL5. IL6. MI7. OH8. PA9. NC10. NJ[ 45 others ]\n      10325(10.3%)9320(9.3%)4808(4.8%)4377(4.3%)4245(4.2%)3736(3.7%)3610(3.6%)2990(3.0%)2693(2.7%)2595(2.6%)52030(51.7%)\n      \n      0\n(0.0%)\n    \n    \n      LZIP\n[character]\n      1. 857102. 104563. 853644. 785215. 785726. 785777. 007318. 104579. 7853910. 60623[ 22526 others ]\n      53(0.1%)45(0.0%)44(0.0%)43(0.0%)42(0.0%)41(0.0%)39(0.0%)37(0.0%)37(0.0%)36(0.0%)100312(99.6%)\n      \n      0\n(0.0%)\n    \n    \n      LZIP4\n[character]\n      1. 88882. 11993. 12994. 98015. 20996. 13997. 16998. 15999. 149910. 1899[ 8615 others ]\n      899(1.5%)113(0.2%)111(0.2%)106(0.2%)104(0.2%)101(0.2%)100(0.2%)99(0.2%)94(0.2%)89(0.2%)57411(96.9%)\n      \n      41502\n(41.2%)\n    \n    \n      PHONE\n[character]\n      1. (505)880-37442. (520)225-60603. (505)721-10514. (480)461-40005. (972)316-36636. (505)527-58007. (520)745-45888. (480)497-33009. (623)445-500010. (480)484-6100[ 91818 others ]\n      141(0.1%)63(0.1%)36(0.0%)35(0.0%)34(0.0%)33(0.0%)33(0.0%)29(0.0%)28(0.0%)27(0.0%)100270(99.5%)\n      \n      0\n(0.0%)\n    \n    \n      GSLO\n[character]\n      1. PK2. KG3. 094. 065. 076. 057. 038. 049. M10. 01[ 8 others ]\n      31179(31.0%)23839(23.7%)16627(16.5%)12912(12.8%)5441(5.4%)2578(2.6%)1581(1.6%)1165(1.2%)1113(1.1%)964(1.0%)3330(3.3%)\n      \n      0\n(0.0%)\n    \n    \n      GSHI\n[character]\n      1. 052. 123. 084. 065. 046. 027. 038. PK9. M10. N[ 9 others ]\n      28039(27.8%)26443(26.3%)21860(21.7%)10873(10.8%)3938(3.9%)1591(1.6%)1446(1.4%)1430(1.4%)1113(1.1%)796(0.8%)3200(3.2%)\n      \n      0\n(0.0%)\n    \n    \n      VIRTUAL\n[character]\n      1. A virtual school2. Missing3. Not a virtual school4. Not Applicable\n      656(0.7%)183(0.2%)99049(98.3%)841(0.8%)\n      \n      0\n(0.0%)\n    \n    \n      TOTFRL\n[numeric]\n      Mean (sd) : 249.4 (275.2)min ≤ med ≤ max:-9 ≤ 178 ≤ 9626IQR (CV) : 297 (1.1)\n      1906 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FRELCH\n[numeric]\n      Mean (sd) : 221.6 (253.9)min ≤ med ≤ max:-9 ≤ 149 ≤ 7581IQR (CV) : 272 (1.1)\n      1765 distinct values\n      \n      0\n(0.0%)\n    \n    \n      REDLCH\n[numeric]\n      Mean (sd) : 26 (36.9)min ≤ med ≤ max:-9 ≤ 16 ≤ 2045IQR (CV) : 37 (1.4)\n      399 distinct values\n      \n      0\n(0.0%)\n    \n    \n      PK\n[numeric]\n      Mean (sd) : 34.8 (53.5)min ≤ med ≤ max:0 ≤ 22 ≤ 1912IQR (CV) : 43 (1.5)\n      468 distinct values\n      \n      64621\n(64.2%)\n    \n    \n      KG\n[numeric]\n      Mean (sd) : 65 (46.9)min ≤ med ≤ max:0 ≤ 62 ≤ 948IQR (CV) : 57 (0.7)\n      393 distinct values\n      \n      43684\n(43.4%)\n    \n    \n      G01\n[numeric]\n      Mean (sd) : 64.4 (44.8)min ≤ med ≤ max:0 ≤ 62 ≤ 1408IQR (CV) : 56 (0.7)\n      353 distinct values\n      \n      43333\n(43.0%)\n    \n    \n      G02\n[numeric]\n      Mean (sd) : 64.6 (44.4)min ≤ med ≤ max:0 ≤ 63 ≤ 688IQR (CV) : 56 (0.7)\n      345 distinct values\n      \n      43268\n(43.0%)\n    \n    \n      G03\n[numeric]\n      Mean (sd) : 66.4 (46.3)min ≤ med ≤ max:0 ≤ 64 ≤ 783IQR (CV) : 59 (0.7)\n      358 distinct values\n      \n      43253\n(42.9%)\n    \n    \n      G04\n[numeric]\n      Mean (sd) : 67.9 (48.7)min ≤ med ≤ max:0 ≤ 65 ≤ 877IQR (CV) : 61 (0.7)\n      382 distinct values\n      \n      43470\n(43.2%)\n    \n    \n      G05\n[numeric]\n      Mean (sd) : 69.7 (56.7)min ≤ med ≤ max:0 ≤ 64 ≤ 985IQR (CV) : 65 (0.8)\n      494 distinct values\n      \n      44673\n(44.3%)\n    \n    \n      G06\n[numeric]\n      Mean (sd) : 91.5 (108.4)min ≤ med ≤ max:0 ≤ 56 ≤ 1155IQR (CV) : 111 (1.2)\n      641 distinct values\n      \n      58585\n(58.2%)\n    \n    \n      G07\n[numeric]\n      Mean (sd) : 102.7 (126.2)min ≤ med ≤ max:0 ≤ 52 ≤ 1439IQR (CV) : 153 (1.2)\n      687 distinct values\n      \n      63682\n(63.2%)\n    \n    \n      G08\n[numeric]\n      Mean (sd) : 101.9 (127.1)min ≤ med ≤ max:0 ≤ 50 ≤ 1608IQR (CV) : 152 (1.2)\n      700 distinct values\n      \n      63449\n(63.0%)\n    \n    \n      G09\n[numeric]\n      Mean (sd) : 124.7 (185.8)min ≤ med ≤ max:0 ≤ 40 ≤ 2799IQR (CV) : 166 (1.5)\n      987 distinct values\n      \n      68499\n(68.0%)\n    \n    \n      G10\n[numeric]\n      Mean (sd) : 120.4 (178.1)min ≤ med ≤ max:0 ≤ 39 ≤ 1837IQR (CV) : 157 (1.5)\n      945 distinct values\n      \n      68706\n(68.2%)\n    \n    \n      G11\n[numeric]\n      Mean (sd) : 115.4 (170.1)min ≤ med ≤ max:0 ≤ 40 ≤ 1719IQR (CV) : 149 (1.5)\n      914 distinct values\n      \n      68720\n(68.2%)\n    \n    \n      G12\n[numeric]\n      Mean (sd) : 114.1 (165.5)min ≤ med ≤ max:0 ≤ 43 ≤ 2580IQR (CV) : 150 (1.5)\n      891 distinct values\n      \n      68814\n(68.3%)\n    \n    \n      G13\n[logical]\n      1. FALSE2. TRUE\n      36(97.3%)1(2.7%)\n      \n      100692\n(100.0%)\n    \n    \n      TOTAL\n[numeric]\n      Mean (sd) : 515.7 (450.2)min ≤ med ≤ max:0 ≤ 434 ≤ 14286IQR (CV) : 408 (0.9)\n      2945 distinct values\n      \n      2229\n(2.2%)\n    \n    \n      MEMBER\n[numeric]\n      Mean (sd) : 515.6 (449.9)min ≤ med ≤ max:0 ≤ 434 ≤ 14286IQR (CV) : 408 (0.9)\n      2944 distinct values\n      \n      2229\n(2.2%)\n    \n    \n      AM\n[numeric]\n      Mean (sd) : 6.7 (30.3)min ≤ med ≤ max:0 ≤ 1 ≤ 1395IQR (CV) : 4 (4.5)\n      424 distinct values\n      \n      20609\n(20.5%)\n    \n    \n      HI\n[numeric]\n      Mean (sd) : 142.5 (240.6)min ≤ med ≤ max:0 ≤ 49 ≤ 4677IQR (CV) : 160 (1.7)\n      1745 distinct values\n      \n      3852\n(3.8%)\n    \n    \n      BL\n[numeric]\n      Mean (sd) : 83 (151.4)min ≤ med ≤ max:0 ≤ 19 ≤ 5088IQR (CV) : 90 (1.8)\n      1166 distinct values\n      \n      8325\n(8.3%)\n    \n    \n      WH\n[numeric]\n      Mean (sd) : 247.9 (275.1)min ≤ med ≤ max:0 ≤ 182 ≤ 8146IQR (CV) : 312 (1.1)\n      1839 distinct values\n      \n      3993\n(4.0%)\n    \n    \n      HP\n[numeric]\n      Mean (sd) : 3.1 (24.7)min ≤ med ≤ max:0 ≤ 0 ≤ 1394IQR (CV) : 2 (8)\n      305 distinct values\n      \n      30008\n(29.8%)\n    \n    \n      TR\n[numeric]\n      Mean (sd) : 20.7 (27.3)min ≤ med ≤ max:0 ≤ 12 ≤ 1228IQR (CV) : 24 (1.3)\n      307 distinct values\n      \n      7137\n(7.1%)\n    \n    \n      FTE\n[numeric]\n      Mean (sd) : 32.6 (25.6)min ≤ med ≤ max:0 ≤ 27.6 ≤ 1419IQR (CV) : 24 (0.8)\n      10066 distinct values\n      \n      5233\n(5.2%)\n    \n    \n      LATCOD\n[numeric]\n      Mean (sd) : 37.8 (5.8)min ≤ med ≤ max:-14.3 ≤ 38.8 ≤ 71.3IQR (CV) : 7.7 (0.2)\n      96746 distinct values\n      \n      0\n(0.0%)\n    \n    \n      LONCOD\n[numeric]\n      Mean (sd) : -92.9 (16.9)min ≤ med ≤ max:-176.6 ≤ -89.3 ≤ 144.9IQR (CV) : 20.2 (-0.2)\n      96911 distinct values\n      \n      0\n(0.0%)\n    \n    \n      ULOCALE\n[character]\n      1. 21-Suburb: Large2. 11-City: Large3. 41-Rural: Fringe4. 42-Rural: Distant5. 13-City: Small6. 43-Rural: Remote7. 32-Town: Distant8. 12-City: Mid-size9. 33-Town: Remote10. 22-Suburb: Mid-size[ 2 others ]\n      26772(26.6%)14851(14.7%)11179(11.1%)10279(10.2%)6635(6.6%)6412(6.4%)6266(6.2%)5876(5.8%)4138(4.1%)3305(3.3%)5016(5.0%)\n      \n      0\n(0.0%)\n    \n    \n      STUTERATIO\n[numeric]\n      Mean (sd) : 16.9 (85.7)min ≤ med ≤ max:0 ≤ 15.3 ≤ 22350IQR (CV) : 5.3 (5.1)\n      3854 distinct values\n      \n      6835\n(6.8%)\n    \n    \n      STITLEI\n[character]\n      1. Missing2. No3. Not Applicable4. Yes\n      864(0.9%)14596(14.5%)29199(29.0%)56070(55.7%)\n      \n      0\n(0.0%)\n    \n    \n      AMALM\n[numeric]\n      Mean (sd) : 3.7 (16.1)min ≤ med ≤ max:0 ≤ 1 ≤ 743IQR (CV) : 2 (4.4)\n      268 distinct values\n      \n      26365\n(26.2%)\n    \n    \n      AMALF\n[numeric]\n      Mean (sd) : 3.6 (15.5)min ≤ med ≤ max:0 ≤ 1 ≤ 652IQR (CV) : 2 (4.4)\n      263 distinct values\n      \n      26708\n(26.5%)\n    \n    \n      ASALM\n[numeric]\n      Mean (sd) : 15.9 (45.2)min ≤ med ≤ max:0 ≤ 3 ≤ 1997IQR (CV) : 11 (2.8)\n      522 distinct values\n      \n      16162\n(16.0%)\n    \n    \n      ASALF\n[numeric]\n      Mean (sd) : 15.1 (42.5)min ≤ med ≤ max:0 ≤ 3 ≤ 1532IQR (CV) : 11 (2.8)\n      495 distinct values\n      \n      16080\n(16.0%)\n    \n    \n      HIALM\n[numeric]\n      Mean (sd) : 73.7 (123.5)min ≤ med ≤ max:0 ≤ 25 ≤ 2292IQR (CV) : 83 (1.7)\n      1073 distinct values\n      \n      4774\n(4.7%)\n    \n    \n      HIALF\n[numeric]\n      Mean (sd) : 70.5 (118.7)min ≤ med ≤ max:0 ≤ 24 ≤ 2461IQR (CV) : 79 (1.7)\n      1047 distinct values\n      \n      5121\n(5.1%)\n    \n    \n      BLALM\n[numeric]\n      Mean (sd) : 43.5 (77.3)min ≤ med ≤ max:0 ≤ 11 ≤ 2473IQR (CV) : 48 (1.8)\n      687 distinct values\n      \n      10801\n(10.7%)\n    \n    \n      BLALF\n[numeric]\n      Mean (sd) : 42.1 (76.8)min ≤ med ≤ max:0 ≤ 10 ≤ 2615IQR (CV) : 46 (1.8)\n      693 distinct values\n      \n      11485\n(11.4%)\n    \n    \n      WHALM\n[numeric]\n      Mean (sd) : 128.6 (140.5)min ≤ med ≤ max:0 ≤ 95 ≤ 3854IQR (CV) : 160 (1.1)\n      1046 distinct values\n      \n      4502\n(4.5%)\n    \n    \n      WHALF\n[numeric]\n      Mean (sd) : 120.8 (135.6)min ≤ med ≤ max:0 ≤ 88 ≤ 4292IQR (CV) : 152 (1.1)\n      1030 distinct values\n      \n      4682\n(4.6%)\n    \n    \n      HPALM\n[numeric]\n      Mean (sd) : 1.7 (13.4)min ≤ med ≤ max:0 ≤ 0 ≤ 751IQR (CV) : 1 (7.9)\n      210 distinct values\n      \n      34182\n(33.9%)\n    \n    \n      HPALF\n[numeric]\n      Mean (sd) : 1.6 (12.2)min ≤ med ≤ max:0 ≤ 0 ≤ 643IQR (CV) : 1 (7.7)\n      212 distinct values\n      \n      34563\n(34.3%)\n    \n    \n      TRALM\n[numeric]\n      Mean (sd) : 10.8 (13.9)min ≤ med ≤ max:0 ≤ 6 ≤ 512IQR (CV) : 13 (1.3)\n      174 distinct values\n      \n      9200\n(9.1%)\n    \n    \n      TRALF\n[numeric]\n      Mean (sd) : 10.5 (14)min ≤ med ≤ max:0 ≤ 6 ≤ 716IQR (CV) : 12 (1.3)\n      183 distinct values\n      \n      9477\n(9.4%)\n    \n    \n      TOTMENROL\n[numeric]\n      Mean (sd) : 264.9 (229)min ≤ med ≤ max:0 ≤ 224 ≤ 6890IQR (CV) : 210 (0.9)\n      1691 distinct values\n      \n      2296\n(2.3%)\n    \n    \n      TOTFENROL\n[numeric]\n      Mean (sd) : 251.1 (222.8)min ≤ med ≤ max:0 ≤ 211 ≤ 7396IQR (CV) : 200 (0.9)\n      1646 distinct values\n      \n      2362\n(2.3%)\n    \n    \n      STATUS\n[numeric]\n      Mean (sd) : 1.1 (0.6)min ≤ med ≤ max:1 ≤ 1 ≤ 8IQR (CV) : 0 (0.5)\n      1:98557(97.8%)3:1103(1.1%)4:77(0.1%)5:110(0.1%)6:500(0.5%)7:341(0.3%)8:41(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      UG\n[numeric]\n      Mean (sd) : 11.2 (33.6)min ≤ med ≤ max:0 ≤ 2 ≤ 1017IQR (CV) : 10 (3)\n      217 distinct values\n      \n      88689\n(88.0%)\n    \n    \n      AE\n[logical]\n      1. FALSE2. TRUE\n      60(93.8%)4(6.2%)\n      \n      100665\n(99.9%)\n    \n    \n      SCHOOL_TYPE_TEXT\n[character]\n      1. Alternative/other school2. Regular school3. Special education school4. Vocational school\n      5531(5.5%)91737(91.1%)1948(1.9%)1513(1.5%)\n      \n      0\n(0.0%)\n    \n    \n      SY_STATUS_TEXT\n[character]\n      1. Currently operational2. New school3. School has changed agency4. School has reopened5. School temporarily closed6. School to be operational 7. School was operational bu\n      98557(97.8%)1103(1.1%)110(0.1%)41(0.0%)500(0.5%)341(0.3%)77(0.1%)\n      \n      0\n(0.0%)\n    \n    \n      SCHOOL_LEVEL\n[character]\n      1. Adult Education2. Elementary3. High4. Middle5. Not Applicable6. Not Reported7. Other8. Prekindergarten9. Secondary10. Ungraded\n      28(0.0%)53287(52.9%)22977(22.8%)16506(16.4%)796(0.8%)1113(1.1%)3824(3.8%)1430(1.4%)602(0.6%)166(0.2%)\n      \n      0\n(0.0%)\n    \n    \n      AS\n[numeric]\n      Mean (sd) : 29.8 (85.8)min ≤ med ≤ max:0 ≤ 5 ≤ 3529IQR (CV) : 21 (2.9)\n      850 distinct values\n      \n      12717\n(12.6%)\n    \n    \n      CHARTER_TEXT\n[character]\n      1. No2. Not Applicable3. Yes\n      87007(86.4%)6387(6.3%)7335(7.3%)\n      \n      0\n(0.0%)\n    \n    \n      MAGNET_TEXT\n[character]\n      1. Missing2. No3. Not Applicable4. Yes\n      6256(6.2%)77531(77.0%)13520(13.4%)3422(3.4%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-23\n\n\n\n\nUnivariate Visualizations\n\n\nBivariate Visualization(s)\n\n\n\n\nUnivariate Visualizations\n\n\nBivariate Visualization(s)"
  },
  {
    "objectID": "posts/challenge1_solutions.html",
    "href": "posts/challenge1_solutions.html",
    "title": "Challenge 1 Solution",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/challenge1_solutions.html#working-with-tabular-data",
    "href": "posts/challenge1_solutions.html#working-with-tabular-data",
    "title": "Challenge 1 Solution",
    "section": "Working with Tabular Data",
    "text": "Working with Tabular Data\nOur advanced datasets ( ⭐⭐⭐ and higher) are tabular data (i.e., tables) that are often published based on government sources or by other organizations. Tabular data is often made available in Excel format (.xls or .xlsx) and is formatted for ease of reading - but this can make it tricky to read into R and reshape into a usable dataset.\nReading in tabular data will follow the same general work flow or work process regardless of formatting differences. We will work through the steps in detail this week (and in future weeks as new datasets are introduced), but this is an outline of the basic process. Note that not every step is needed for every file.\n\nIdentify grouping variables and values to extract from the table\nIdentify formatting issues that need to be addressed or eliminated\nIdentify column issues to be addressed during data read-in\nChoose column names to allow pivoting or future analysis\nAddress issues in rows using filter (and stringr package)\nCreate or mutate new variables as required, using separate, pivot_longer, etc\n\n\nRailroad ⭐FAOSTAT ⭐⭐Wild Birds ⭐⭐⭐Railroad (xls) ⭐⭐⭐⭐\n\n\nIt is hard to get much information about the data source or contents from a .csv file - as compared to the formatted .xlsx version of the same data described below.\n\nRead the Data\n\n\nCode\nrailroad<-read_csv(\"_data/railroad_2012_clean_county.csv\")\n\n\nRows: 2930 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): state, county\ndbl (1): total_employees\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nrailroad\n\n\n\n\n  \n\n\n\nFrom inspection, we can that the three variables are named state, county, and total employees. Combined with the name of the fail, this appears to be the aggregated data on the number of employees working for the railroad in each county 2012. We assume that the 2930 cases - which are counties embedded within states1 - consist only of counties where there are railroad employees?\n\n\nCode\nrailroad%>%\n  select(state)%>%\n  n_distinct(.)\n\n\n[1] 53\n\n\nCode\nrailroad%>%\n  select(state)%>%\n  distinct()\n\n\n\n\n  \n\n\n\nWith a few simple commands, we can confirm that there are 53 “states” represented in the data. To identify the additional non-state areas (probably District of Columbia, plus some combination of Puerto Rico and/or overseas addresses), we can print out a list of unique state names.\n\n1: We can identify case variables because both are character variables, which in tidy lingo are grouping variables not values.\n\n\n\nOnce again, a .csv file lacks any of the additional information that might be present in a published Excel table. So, we know the data are likely to be about birds, but will we be looking at individual pet birds, prices of bird breeds sold in stores, the average flock size of wild birds - who knows!\nThe FAOSTAT*.csv files have some additional information - the FAO - which a Google search reveals to be the Food and Agriculture Association of the United Nations publishes country-level data regularly in a database called FAOSTAT. So my best guess at this point is that we are going to be looking at country-level estaimtes of the number of birds that are raised for eggs and poultry, but we will see if this is right by inspecting the data.\n\nRead the Data\n\n\nCode\nbirds<-read_csv(\"_data/birds.csv\")\n\n\nRows: 30977 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Domain Code, Domain, Area, Element, Item, Unit, Flag, Flag Description\ndbl (6): Area Code, Element Code, Item Code, Year Code, Year, Value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nbirds\n\n\n\n\n  \n\n\n\nCode\nchickens<-read_csv(\"_data/FAOSTAT_egg_chicken.csv\")\n\n\nRows: 38170 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Domain Code, Domain, Area, Element, Item, Unit, Flag, Flag Description\ndbl (6): Area Code, Element Code, Item Code, Year Code, Year, Value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nchickens\n\n\n\n\n  \n\n\n\nIt is pretty difficult to get a handle on what data are being captured by any of the FAOSTAT* (including the birds.csv) data sets simply from a quick scan of the tibble after read in. It was easy with the railroad data, but now we are going to have to work harder to describe exactly what comprises a case in these data and what values are present for each case. We can see that there are 30,970 rows in the birds data (and 38,170 rows in the chickens) - but this might not mean that there are 30,970 (or 38,170) cases because we aren’t sure what constitutes a case at this point.\n\n\nWhat is a case?\nOne approach to figuring out what constitutes a case is to identify the value variables and assume that what is leftover are the grouping variables. Unfortunately, there are six double variables (from the column descriptions that are automatically returned), and it appears that most of them are not grouping variables. For example, the variable “Area Code” is a double - but doesn’t appear to be a value that varies across rows. Thus, it is a grouping variable rather than a true value in tidy nomenclature. Similar issues can be found with Year and “Item Code” - both appear to be grouping variables. Ironically, it is the variable called Value which appears to the sole value in the data set - but what is it the value of?\nAnother approach to identifying a case is to look for variation (or lack of variation) in just the first few cases of the tibble. (Think of this as the basis for a minimal reproducible example.) In the first few cases, the variables of the first 10 cases appear to be identical until we get to Year and Year Code (which appear to be identical to each other.) So it appears that Value is varying by country-year - but perhaps also by information in one of the other variables. It also appears that many of the doubles are just numeric codes, so lets drop those variables to simplify (I’m going to drop down to just showing the birds data for now.)\n\n\nCode\nbirds.sm<-birds%>%\n  select(-contains(\"Code\"))\nbirds.sm\n\n\n\n\n  \n\n\n\nCode\nchickens.sm<-chickens%>%\n  select(-contains(\"Code\"))\n\n\n\n\nVisual Summary of Data Set\nBefore we go doing detailed cross-tabs to figure out where there is variation, lets do a high level summary of the dataset to see if - for example - there are multiple values in the Element variable - or if we only have a dataset with records containing estimates of Chicken Stocks (from Element + Item.)\nTo get a better grasp of the data, lets do a quick skim or summary of the dataset and see if we can find out more about our data at a glance. I am using the dfSummary function from the summarytools package -one of the more attractive ways to quickly summarise a dataset. I am using a few options to allow it to render directly to html.\n\n\nCode\nprint(summarytools::dfSummary(birds.sm,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nbirds.sm\nDimensions: 30977 x 9\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      Domain\n[character]\n      1. Live Animals\n      30977(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Area\n[character]\n      1. Africa2. Asia3. Eastern Asia4. Egypt5. Europe6. France7. Greece8. Myanmar9. Northern Africa10. South-eastern Asia[ 238 others ]\n      290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)28077(90.6%)\n      \n      0\n(0.0%)\n    \n    \n      Element\n[character]\n      1. Stocks\n      30977(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Item\n[character]\n      1. Chickens2. Ducks3. Geese and guinea fowls4. Pigeons, other birds5. Turkeys\n      13074(42.2%)6909(22.3%)4136(13.4%)1165(3.8%)5693(18.4%)\n      \n      0\n(0.0%)\n    \n    \n      Year\n[numeric]\n      Mean (sd) : 1990.6 (16.7)min ≤ med ≤ max:1961 ≤ 1992 ≤ 2018IQR (CV) : 29 (0)\n      58 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Unit\n[character]\n      1. 1000 Head\n      30977(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Value\n[numeric]\n      Mean (sd) : 99410.6 (720611.4)min ≤ med ≤ max:0 ≤ 1800 ≤ 23707134IQR (CV) : 15233 (7.2)\n      11495 distinct values\n      \n      1036\n(3.3%)\n    \n    \n      Flag\n[character]\n      1. *2. A3. F4. Im5. M\n      1494(7.4%)6488(32.1%)10007(49.5%)1213(6.0%)1002(5.0%)\n      \n      10773\n(34.8%)\n    \n    \n      Flag Description\n[character]\n      1. Aggregate, may include of2. Data not available3. FAO data based on imputat4. FAO estimate5. Official data6. Unofficial figure\n      6488(20.9%)1002(3.2%)1213(3.9%)10007(32.3%)10773(34.8%)1494(4.8%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-22\n\n\n\nFinally - we have a much better grasp on what is going on. First, we know that all records in this data set are of the number of Live Animal Stocks (Domain + Element), with the value expressed as 1000 heads (Unit). These three variables are grouping variables but DO NOT vary in this particular data extract - but are probably used to create data extracts from the larger FAOSTAT database.. To see if we are correct, we will have to checkout the same fields in the chickens data below.\nSecond, we can now guess that a case consists of a country-year-animal record - as captured in the variables Area, Year and Item, respectively - estimate of the number of live animals (Value.) ALso, as a side note, it appears that the estimated number of animals may have a long right-hand tail - just looking at the mini-histogram. So we can now say that we have estimates of the stock of five different types of poultry (Chickens, Ducks, Geese and guinea fowls, Turkeys, and Pigeons/Others) in 248 areas (countries??) for 58 years between 1961-2018.\nThe only minor concern is that we are still not entirely sure what information is being captured in the Flag (and matching Flag Description) variable. It appears unlikely that there is more than one estimate per country-year-animal case (see the summary of Area where all countries have 290 observations.) An assumption of one type of estimate (the content of Flag Description) per year is also consistent with the histogram of Year, which is pretty consistent although more countries were clearly added later in the series and data collection is not complete for the most recent time period.\nWe can dig a bit more, and find the description of the Flag field on the FAOSTAT website.. Sure enough, this confirms that the flags correspond to what type of estimate is being used (e.g., official data vs an estimate by FAOSTAT or imputed data.)\nWe can also confirm that NOT all cases are countries, as there is a Flag value, A, described as aggregated data. A quick inspection of the areas using this flag confirm that all of the “countries” are actually regional aggregations, and should be filtered out of the dataset as they are not the same “type” of case as a country-level case. To fix these data into true tidy format, we would need to filter out the aggregates, then merge on the country group definitions from FAOSTAT to create new country-group or regional variables that could be used to recreate aggregated estimates with dplyr.\n\n\nCode\nbirds.sm%>%\n  filter(Flag==\"A\")%>%\n  group_by(Area)%>%\n  summarize(n=n())\n\n\n\n\n  \n\n\n\n\n\nFAOstat*.csv\nLets take a quick look at our chickens data to see if it follows the same basic pattern as the birds data. Sure enough, it looks like we have a different domain (livestock products) but that the cases remain similar country-year-product, with three slightly different estimates related to egg-laying (instead of the five types of poultry.)\n\n\nCode\nprint(summarytools::dfSummary(chickens.sm,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nchickens.sm\nDimensions: 38170 x 9\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      Domain\n[character]\n      1. Livestock Primary\n      38170(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Area\n[character]\n      1. Afghanistan2. Africa3. Albania4. Algeria5. American Samoa6. Americas7. Angola8. Antigua and Barbuda9. Argentina10. Asia[ 235 others ]\n      174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)36430(95.4%)\n      \n      0\n(0.0%)\n    \n    \n      Element\n[character]\n      1. Laying2. Production3. Yield\n      12679(33.2%)12840(33.6%)12651(33.1%)\n      \n      0\n(0.0%)\n    \n    \n      Item\n[character]\n      1. Eggs, hen, in shell\n      38170(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Year\n[numeric]\n      Mean (sd) : 1990.5 (16.7)min ≤ med ≤ max:1961 ≤ 1991 ≤ 2018IQR (CV) : 29 (0)\n      58 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Unit\n[character]\n      1. 1000 Head2. 100mg/An3. tonnes\n      12679(33.2%)12651(33.1%)12840(33.6%)\n      \n      0\n(0.0%)\n    \n    \n      Value\n[numeric]\n      Mean (sd) : 291341.2 (2232761)min ≤ med ≤ max:1 ≤ 31996 ≤ 76769955IQR (CV) : 91235.8 (7.7)\n      21325 distinct values\n      \n      40\n(0.1%)\n    \n    \n      Flag\n[character]\n      1. *2. A3. F4. Fc5. Im6. M\n      1435(4.7%)3186(10.4%)10538(34.4%)13344(43.6%)2079(6.8%)40(0.1%)\n      \n      7548\n(19.8%)\n    \n    \n      Flag Description\n[character]\n      1. Aggregate, may include of2. Calculated data3. Data not available4. FAO data based on imputat5. FAO estimate6. Official data7. Unofficial figure\n      3186(8.3%)13344(35.0%)40(0.1%)2079(5.4%)10538(27.6%)7548(19.8%)1435(3.8%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-22\n\n\n\n\n\n\nThe “wild_bird_data” sheet is in Excel format (.xlsx) instead of the .csv format of the earlier data sets. In theory, it should be no harder to read in an Excel worksheet (or even workbook) as compared to a .csv file - there is a package called read_xl that is part of the tidyverse that easily reads in excel files.\nHowever, in practice, most people use Excel sheets as a publication format - not a way to store data, so there is almost always a ton of “junk” in the file that is NOT part of the data table that we want to read in. Sometimes the additional “junk” is incredibly useful - it might include table notes or information about data sources. However, we still need a systematic way to identify this junk and get rid of it during the data reading step.\nFor example, lets see what happens here if we just read in the wild bird data straight from excel.\n\n\nCode\nwildbirds<-read_excel(\"_data/wild_bird_data.xlsx\")\nwildbirds\n\n\n\n\n  \n\n\n\nHm, this doesn’t seem quite right. It is clear that the first “case” has information in it that looks more like variable labels. Lets take a quick look at the raw data.\n\n\n\nWild Bird Excel File\n\n\nSure enough the Excel file first row does contain additional information, a pointer to the article that this data was drawn from, and a quick Google reveals the article is [Nee, S., Read, A., Greenwood, J. et al. The relationship between abundance and body size in British birds. Nature 351, 312–313 (1991)] (https://www.nature.com/articles/351312a0)\n\nSkipping a row\nWe could try to manually adjust things - remove the first row, change the column names, and then change the column types. But this is both a lot of work, and not really a best practice for data management. Lets instead re-read the data in with the skip option from read_excel, and see if it fixes all of our problems!\n\n\nCode\nwildbirds <- read_excel(\"_data/wild_bird_data.xlsx\",\n                        skip = 1)\nwildbirds\n\n\n\n\n  \n\n\n\nThis now looks great! Both variables are numeric, and now they correctly show up as double or (). The variable names might be a bit tough to work with, though, so it can be easier to assign new column names on the read in - and then manually adjust axis labels, etc once you are working on your publication-quality graphs.\nNote that I skip two rows this time, and apply my own column names.\n\n\nCode\nwildbirds <- read_excel(\"_data/wild_bird_data.xlsx\",\n                        skip = 2, \n                        col_names = c(\"weight\", \"pop_size\"))\nwildbirds\n\n\n\n\n  \n\n\n\n\n\n\nThe railroad data set is our most challenging data to read in this week, but is (by comparison) a fairly straightforward formatted table published by the Railroad Retirement Board. The value variable is a count of the number of employees in each county and state combination. \nLooking at the excel file, we can see that there are only a few issues: 1. There are three rows at the top of the sheet that are not needed 2. There are blank columns that are not needed. 3. There are Total rows for each state that are not needed\n\nSkipping title rows\nFor the first issue, we use the “skip” option on read_excel from the readxl package to skip the rows at the top.\n\n\nCode\nread_excel(\"_data/StateCounty2012.xls\",\n                     skip = 3)\n\n\nNew names:\n• `` -> `...2`\n• `` -> `...4`\n\n\n\n\n  \n\n\n\n\n\nRemoving empty columns\nFor the second issue, I name the blank columns “delete” to make is easy to remove the unwanted columns. I then use select (with the ! sign to designate the complement or NOT) to select columns we wish to keep in the dataset - the rest are removed. Note that I skip 4 rows this time as I do not need the original header row.\nThere are other approaches you could use for this task (e.g., remove all columns that have no valid volues), but hard coding of variable names and types during data read in is not considered a violation of best practices and - if used strategically - can often make later data cleaning much easier.\n\n\nCode\nread_excel(\"_data/StateCounty2012.xls\",\n                     skip = 4,\n                     col_names= c(\"State\", \"delete\", \"County\", \"delete\", \"Employees\"))%>%\n  select(!contains(\"delete\"))\n\n\nNew names:\n• `delete` -> `delete...2`\n• `delete` -> `delete...4`\n\n\n\n\n  \n\n\n\n\n\nFiltering “total” rows\nFor the third issue, we are going to use filter to identify (and drop the rows that have the word “Total” in the State column). str_detect can be used to find specific rows within a column that have the designated “pattern”, while the “!” designates the complement of the selected rows (i.e., those without the “pattern” we are searching for.)\nThe str_detect command is from the stringr package, and is a powerful and easy to use implementation of grep and regex in the tidyverse - the base R functions (grep, gsub, etc) are classic but far more difficult to use, particularly for those not in practice. Be sure to explore the stringr package on your own.\n\n\nCode\nrailroad<-read_excel(\"_data/StateCounty2012.xls\",\n                     skip = 4,\n                     col_names= c(\"State\", \"delete\", \"County\", \"delete\", \"Employees\"))%>%\n  select(!contains(\"delete\"))%>%\n  filter(!str_detect(State, \"Total\"))\n\n\nNew names:\n• `delete` -> `delete...2`\n• `delete` -> `delete...4`\n\n\nCode\nrailroad\n\n\n\n\n  \n\n\n\n\n\nRemove any table notes\nTables often have notes in the last few table rows. You can check table limits and use this information during data read-in to not read the notes by setting the n-max option at the total number of rows to read, or less commonly, the range option to specify the spreadsheet range in standard excel naming (e.g., “B4:R142”). If you didn’t handle this on read in, you can use the tail command to check for notes and either tail or head to keep only the rows that you need.\n\n\nCode\ntail(railroad, 10)\n\n\n\n\n  \n\n\n\nCode\n#remove the last two observations\nrailroad <-head(railroad, -2)\ntail(railroad, 10)\n\n\n\n\n  \n\n\n\n\n\nConfirm cases\nAnd that is all it takes! The data are now ready for analysis. Lets see if we get the same number of unique states that were in the cleaned data in exercise 1.\n\n\nCode\nrailroad%>%\n  select(State)%>%\n  n_distinct(.)\n\n\n[1] 54\n\n\nCode\nrailroad%>%\n  select(State)%>%\n  distinct()\n\n\n\n\n  \n\n\n\nOh my goodness! It seems that we have an additional “State” - it looks like Canada is in the full excel data and not the tidy data. This is one example of why it is good practice to always work from the original data source!"
  },
  {
    "objectID": "posts/challenge3_instructions.html",
    "href": "posts/challenge3_instructions.html",
    "title": "Challenge 3 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_instructions.html#challenge-overview",
    "href": "posts/challenge3_instructions.html#challenge-overview",
    "title": "Challenge 3 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nidentify what needs to be done to tidy the current data\nanticipate the shape of pivoted data\npivot the data into tidy format using pivot_longer"
  },
  {
    "objectID": "posts/challenge3_instructions.html#read-in-data",
    "href": "posts/challenge3_instructions.html#read-in-data",
    "title": "Challenge 3 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nanimal_weights.csv ⭐\neggs_tidy.csv ⭐⭐ or organicpoultry.xls ⭐⭐⭐\naustralian_marriage*.xlsx ⭐⭐⭐\nUSA Households*.xlsx ⭐⭐⭐⭐\nsce_labor_chart_data_public.csv 🌟🌟🌟🌟🌟\n\n\n\n\n\nBriefly describe the data\nDescribe the data, and be sure to comment on why you are planning to pivot it to make it “tidy”"
  },
  {
    "objectID": "posts/challenge3_instructions.html#anticipate-the-end-result",
    "href": "posts/challenge3_instructions.html#anticipate-the-end-result",
    "title": "Challenge 3 Instructions",
    "section": "Anticipate the End Result",
    "text": "Anticipate the End Result\nThe first step in pivoting the data is to try to come up with a concrete vision of what the end product should look like - that way you will know whether or not your pivoting was successful.\nOne easy way to do this is to think about the dimensions of your current data (tibble, dataframe, or matrix), and then calculate what the dimensions of the pivoted data should be.\nSuppose you have a dataset with \\(n\\) rows and \\(k\\) variables. In our example, 3 of the variables are used to identify a case, so you will be pivoting \\(k-3\\) variables into a longer format where the \\(k-3\\) variable names will move into the names_to variable and the current values in each of those columns will move into the values_to variable. Therefore, we would expect \\(n * (k-3)\\) rows in the pivoted dataframe!\n\nExample: find current and future data dimensions\nLets see if this works with a simple example.\n\n\nCode\ndf<-tibble(country = rep(c(\"Mexico\", \"USA\", \"France\"),2),\n           year = rep(c(1980,1990), 3), \n           trade = rep(c(\"NAFTA\", \"NAFTA\", \"EU\"),2),\n           outgoing = rnorm(6, mean=1000, sd=500),\n           incoming = rlogis(6, location=1000, \n                             scale = 400))\ndf\n\n\n# A tibble: 6 × 5\n  country  year trade outgoing incoming\n  <chr>   <dbl> <chr>    <dbl>    <dbl>\n1 Mexico   1980 NAFTA    1394.    1265.\n2 USA      1990 NAFTA    1114.    1114.\n3 France   1980 EU        497.   -1433.\n4 Mexico   1990 NAFTA     409.     469.\n5 USA      1980 NAFTA    1155.    1570.\n6 France   1990 EU        652.     345.\n\n\nCode\n#existing rows/cases\nnrow(df)\n\n\n[1] 6\n\n\nCode\n#existing columns/cases\nncol(df)\n\n\n[1] 5\n\n\nCode\n#expected rows/cases\nnrow(df) * (ncol(df)-3)\n\n\n[1] 12\n\n\nCode\n# expected columns \n3 + 2\n\n\n[1] 5\n\n\nOr simple example has \\(n = 6\\) rows and \\(k - 3 = 2\\) variables being pivoted, so we expect a new dataframe to have \\(n * 2 = 12\\) rows x \\(3 + 2 = 5\\) columns.\n\n\nChallenge: Describe the final dimensions\nDocument your work here.\n\n\n\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge3_instructions.html#pivot-the-data",
    "href": "posts/challenge3_instructions.html#pivot-the-data",
    "title": "Challenge 3 Instructions",
    "section": "Pivot the Data",
    "text": "Pivot the Data\nNow we will pivot the data, and compare our pivoted data dimensions to the dimensions calculated above as a “sanity” check.\n\nExample\n\n\nCode\ndf<-pivot_longer(df, col = c(outgoing, incoming),\n                 names_to=\"trade_direction\",\n                 values_to = \"trade_value\")\ndf\n\n\n# A tibble: 12 × 5\n   country  year trade trade_direction trade_value\n   <chr>   <dbl> <chr> <chr>                 <dbl>\n 1 Mexico   1980 NAFTA outgoing              1394.\n 2 Mexico   1980 NAFTA incoming              1265.\n 3 USA      1990 NAFTA outgoing              1114.\n 4 USA      1990 NAFTA incoming              1114.\n 5 France   1980 EU    outgoing               497.\n 6 France   1980 EU    incoming             -1433.\n 7 Mexico   1990 NAFTA outgoing               409.\n 8 Mexico   1990 NAFTA incoming               469.\n 9 USA      1980 NAFTA outgoing              1155.\n10 USA      1980 NAFTA incoming              1570.\n11 France   1990 EU    outgoing               652.\n12 France   1990 EU    incoming               345.\n\n\nYes, once it is pivoted long, our resulting data are \\(12x5\\) - exactly what we expected!\n\n\nChallenge: Pivot the Chosen Data\nDocument your work here. What will a new “case” be once you have pivoted the data? How does it meet requirements for tidy data?\n\n\n\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge7_instructions.html",
    "href": "posts/challenge7_instructions.html",
    "title": "Challenge 7 Instructions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge7_instructions.html#challenge-overview",
    "href": "posts/challenge7_instructions.html#challenge-overview",
    "title": "Challenge 7 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nmutate variables as needed (including sanity checks)\nRecreate at least two graphs from previous exercises, but introduce at least one additional dimension that you omitted before using ggplot functionality (color, shape, line, facet, etc) The goal is not to create unneeded chart ink (Tufte), but to concisely capture variation in additional dimensions that were collapsed in your earlier 2 or 3 dimensional graphs.\n\n\nExplain why you choose the specific graph type\n\n\nIf you haven’t tried in previous weeks, work this week to make your graphs “publication” ready with titles, captions, and pretty axis labels and other viewer-friendly features\n\nR Graph Gallery is a good starting point for thinking about what information is conveyed in standard graph types, and includes example R code. And anyone not familiar with Edward Tufte should check out his fantastic books and courses on data visualizaton.\n(be sure to only include the category tags for the data you use!)"
  },
  {
    "objectID": "posts/challenge7_instructions.html#read-in-data",
    "href": "posts/challenge7_instructions.html#read-in-data",
    "title": "Challenge 7 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\neggs ⭐\nabc_poll ⭐⭐\naustralian_marriage ⭐⭐\nhotel_bookings ⭐⭐⭐\nair_bnb ⭐⭐⭐\nus_hh ⭐⭐⭐⭐\nfaostat ⭐⭐⭐⭐⭐\n\n\n\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge7_instructions.html#tidy-data-as-needed",
    "href": "posts/challenge7_instructions.html#tidy-data-as-needed",
    "title": "Challenge 7 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\n\n\n\nAre there any variables that require mutation to be usable in your analysis stream? For example, do you need to calculate new values in order to graph them? Can string values be represented numerically? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here."
  },
  {
    "objectID": "posts/challenge7_instructions.html#visualization-with-multiple-dimensions",
    "href": "posts/challenge7_instructions.html#visualization-with-multiple-dimensions",
    "title": "Challenge 7 Instructions",
    "section": "Visualization with Multiple Dimensions",
    "text": "Visualization with Multiple Dimensions"
  },
  {
    "objectID": "posts/challenge6_solutions.html",
    "href": "posts/challenge6_solutions.html",
    "title": "Challenge 6 Solutions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\nlibrary(readxl)\nlibrary(lubridate)\n\nsource(\"umass_colors.R\")\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge6_solutions.html#challenge-overview",
    "href": "posts/challenge6_solutions.html#challenge-overview",
    "title": "Challenge 6 Solutions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\ncreate at least one graph including time (evolution)\n\n\ntry to make them “publication” ready (optional)\nExplain why you choose the specific graph type\n\n\nCreate at least one graph depicting part-whole or flow relationships\n\n\ntry to make them “publication” ready (optional)\nExplain why you choose the specific graph type\n\n\nDebt ⭐Fed Rates ⭐⭐usa_hh ⭐⭐⭐hotel_bookings ⭐⭐⭐⭐air_bnb ⭐⭐⭐⭐⭐\n\n\nThis data set runs from the first quarter of 2003 to the second quarter of 2021, and includes quarterly measures of the total amount of household debt associated with 6 different types of loans - mortgage,HE revolving, auto, credit card, student, and other - plus a total household debt including all 6 loan types. This is another fantastic macroeconomic data product from the New York Federal Reserve. See Challenge 4.\n\ndebt_orig<-read_excel(\"_data/debt_in_trillions.xlsx\")\ndebt<-debt_orig%>%\n  mutate(date = parse_date_time(`Year and Quarter`, \n                           orders=\"yq\"))\n\n\nTime Dependent Visualization\nLets look at how debt changes over time.\n\nggplot(debt, aes(x=date, y=Total)) +\n  geom_point()\nggplot(debt, aes(x=date, y=Total)) +\n  geom_point(size=.5) +\n  geom_line()+\n  scale_y_continuous(labels = scales::label_number(suffix = \" Trillion\"))\n\n\n\n\n\n\nChange in Total Debt Over Time\n\n\n\n\n\n\n\nChange in Total Debt, v2\n\n\n\n\n\n\n\n\nVisualizing Part-Whole Relationships\nOne thing to note is that it isn’t easy to include multiple lines on a single graph, that is because our data are not pivoted. Here is an example of how pivoting into tidy format makes things super easy.\n\numass_palette<-c(\"red\", \"green\", \"dark blue\", \"light blue\", \"orange\", \n                 \"yellow\")%>%\n                   map(., get_umass_color)%>%\n                   unlist(.)\n\ndebt_long<-debt%>%\n  pivot_longer(cols = Mortgage:Other,\n               names_to = \"Loan\", \n               values_to = \"total\")%>%\n  select(-Total)%>%\n  mutate(Loan = as.factor(Loan))\n\nggplot(debt_long, aes(x=date, y=total, color=Loan)) +\n  geom_point(size=.5) +\n  geom_line() +\n  theme(legend.position = \"right\") +\n  scale_y_continuous(labels = scales::label_number(suffix = \" Trillion\")) +\n  scale_colour_manual(values=umass_palette)\nggplot(debt_long, aes(x=date, y=total, fill=Loan)) +\n  geom_bar(position=\"stack\", stat=\"identity\") +\n  scale_y_continuous(labels = scales::label_number(suffix = \" Trillion\"))+\n  theme(legend.position = \"top\") +\n  guides(fill = guide_legend(nrow = 1)) +\n  scale_fill_manual(labels =\n                      str_replace(levels(debt_long$Loan), \" \", \"\\n\"),\n                      values=umass_palette)\n\n\n\n\n\n\nChange in Debt Over Time, by Debt Type (line)\n\n\n\n\n\n\n\nChange in Debt Over Time, by Debt Type (stacked)\n\n\n\n\n\n\nWhile the stacked chart might be easier to read in some respects, it is harder to follow individual trend lines. One solution is to reorder in order to preserve as much information as possible.\n\ndebt_long<-debt_long%>%\n  mutate(Loan = fct_relevel(Loan, \"Mortgage\", \"HE Revolving\",\n                            \"Auto Loan\", \"Student Loan\",  \n                            \"Credit Card\",\"Other\"))\n\nggplot(debt_long, aes(x=date, y=total, fill=Loan)) +\n  geom_bar(position=\"stack\", stat=\"identity\") +\n  scale_y_continuous(labels = scales::label_number(suffix = \" Trillion\"))+\n  theme(legend.position = \"top\") +\n  guides(fill = guide_legend(nrow = 1)) +\n  scale_fill_manual(labels=\n                      str_replace(levels(debt_long$Loan), \" \", \"\\n\"),\n                      values=umass_palette)\n\n\n\n\n\n\n\nThis data set runs from July 1954 to March 2017, and includes daily macroeconomic indicators related to the effective federal funds rate - or the interest rate at which banks lend money to each other in order to meet mandated reserve requirements. There are 7 variables besides the date: 4 values related to the federal funds rate (target, upper target, lower target, and effective), 3 are related macroeconomic indicators (inflation, GDP change, and unemployment rate.)\n\nfed_rates_vars<-read_csv(\"_data/FedFundsRate.csv\",\n                         n_max = 1,\n                         col_names = NULL)%>%\n  select(-c(X1:X3))%>%\n  unlist(.)\n\nnames(fed_rates_vars) <-c(\"fed_target\", \"fed_target_upper\",\n                         \"fed_target_lower\", \"fed_effective\",\n                         \"gdp_ch\", \"unemploy\", \"inflation\")\n      \nfed_rates_orig<-read_csv(\"_data/FedFundsRate.csv\",\n                         skip=1,\n                         col_names = c(\"Year\", \"Month\", \"Day\", \n                                       names(fed_rates_vars)))\n\nfed_rates<-fed_rates_orig%>%\n  mutate(date = make_date(Year, Month, Day))%>%\n  select(-c(Year, Month, Day))\n\nfed_rates <- fed_rates%>%\n  pivot_longer(cols=-date, \n               names_to = \"variable\",\n               values_to = \"value\")\n\nNow we can try to visualize the data over time, with care paid to missing data.\n\nfed_rates%>%\n  filter(str_starts(variable, \"fed\"))%>%\nggplot(., aes(x=date, y=value, color=variable))+\n  geom_point(size=0)+\n  geom_line()+\n  scale_y_continuous(labels = scales::label_percent(scale = 1))\n\n\n\n\nWe can now see how closely the effective rate adheres to the target rate (and can see how the Fed changed the way it set it target rate around the time of the 2009 financial crash). Can we find out more by comparing the effective rate to one of the other macroeconomic indicators?\n\nfed_rates%>%\n  filter(variable%in%c(\"fed_effective\", \"gdp_ch\", \n                       \"unemploy\", \"inflation\"))%>%\nggplot(., aes(x=date, y=value, color=variable))+\n  geom_point(size=0)+\n  geom_line()+\n  facet_grid(rows = vars(variable))"
  },
  {
    "objectID": "posts/challenge2_solutions.html",
    "href": "posts/challenge2_solutions.html",
    "title": "Challenge 2 Solutions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE,\n                      message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_solutions.html#challenge-overview",
    "href": "posts/challenge2_solutions.html#challenge-overview",
    "title": "Challenge 2 Solutions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics\n\n\nRailroad ⭐FAOstat* ⭐⭐⭐Hotel Bookings ⭐⭐⭐⭐\n\n\nThe railroad data contain 2931 county-level aggregated counts of the number of railroad employees in 2012. Counties are embedded within States, and all 50 states plus Canada, overseas addresses in Asia and Europe, and Washington, DC are represented.\n\n\n\n\n\n\nData Summaries\n\n\n\nThis is a concise summary of more extensive work we did in Challenge 1, and is an example of how you should describe data in public-facing work. A “public-facing” version of your work contains all critical details needed to replicate your work, but doesn’t contain a point-by-point rundown of the mental process you went through to get to that point. (Your internal analysis file should probably walk through that mental process, however!)\n\n\n\nRead the data\nHere, we are just reusing the code from Challenge 1. We are using the excel version, to ensure that we get Canada, and are renaming the missing data in county for Canada so that we don’t accidently filter that observation out.\n\nrailroad<-read_excel(\"_data/StateCounty2012.xls\",\n                     skip = 4,\n                     col_names= c(\"state\", \"delete\",  \"county\",\n                                  \"delete\", \"employees\"))%>%\n  select(!contains(\"delete\"))%>%\n  filter(!str_detect(state, \"Total\"))\n\nrailroad<-head(railroad, -2)%>%\n  mutate(county = ifelse(state==\"CANADA\", \"CANADA\", county))\n\nrailroad\n\n\n\n  \n\n\n\n\n\nHow many values does X take on?\nNow, lets practice grouping our data and using other dplyr commands that make data wrangling super easy. First, lets take a closer look at how we counted the number of unique states last week. First, we selected the state column. Then we used the n_distinct command - which replicates the base R commands length(unique(var)).\n\n\n\n\n\n\nacross()\n\n\n\nInstead of counting the number of distinct values one at a time, I am doing an operation on two columns at the same time using across.\n\n\n\nrailroad%>%\n  summarise(across(c(state,county), n_distinct))\n\n\n\n  \n\n\n\nCheck this out - many counties have the same name! There are 2931 state-county cases, but only 1710 distinct county names. This is one reason it is so critical to understand “what is a case” when you are working with your data - otherwise you might accidentally collapse or group information that isn’t intended to be grouped.\n\n\nHow many total X are in group Y?\nSuppose we want to know the total number of railroad employees was in 2012, what is the best way to sum up all of the values in the data? The summarize function is useful for doing calculations across some or all of a data set.\n\nrailroad%>%\n  summarise(total_employees = sum(employees))\n\n\n\n  \n\n\n\nAround a quarter of a million people were employed in the railroad industry in 2012. While this may seem like a lot, it was a significant decrease in employment from a few decades earlier, according to official Bureau of Labor Statistics (BLS) estimates.\nYou may notice that the BLS estimates are significantly lower than the ones we are using, provided by the Railroad Retirement Board. Given that the Railroad Retirement Board has “gold-standard” data on railroad employees, this discrepancy suggests that many people who work in the railroad industry are being classified in a different way by BLS statistics.\n\n\nWhich X have the most Y?\nSuppose we are interested in which county names are duplicated most often, or in which states have the most railroad employees. We can use the same basic approach to answer both “Which X have the most Y?” style questions\n\n\n\n\n\n\ndf-print: paged (YAML)\n\n\n\nWhen you are using df-print: paged in your yaml header, or are using tibbles, there is no need to rely on the head(data) command to limit your results to the top 10 of a list.\n\n\n\nrailroad%>%\n  group_by(state)%>%\n  summarise(total_employees = sum(employees),\n            num_counties = n())%>%\n  arrange(desc(total_employees))\n\n\n\n  \n\n\n\nLooking at the top 10 states in terms of total railroad employment, a few trends emerge. Several of the top 10 states with geographical activity are highly populous and geographically large. California, Texas, New York, Pennsylvania, Ohio, Illinois, and Georgia are all amonst the top-10 largest states - so it would make sense if there are more railroad employees in large states.\nBut railroads are spread out along geography, and thus we might also expect square mileage within a state to be related to state railroad employment - not just state population. For example, Texas is around 65% larger (in area) than California, and has around 50% more railroad employees.\nThere appear to be multiple exceptions to both rules, however. If geography plus population were the primary factors explaining railroad employment, then California would be ranked higher than New York and Illinois, and New York would likely rank higher than Illinois. However, Illinois - Chicago in particular - is a hub of railroad activity, and thus Illinois’ higher ranking is likely reflecting hub activity and employment. New York is a hub for the East Coast in particular. While California may have hubs of train activity in Los Angeles or San Francisco, the Northeast has a higher density of train stations and almost certainly generates more passenger and freight miles than the larger and more populous California.\nThis final factor - the existence of heavily used train routes probably explains the high railroad employment in states like Nebraska, Indiana and Missouri - all of which lay along a major railway route between New York and Chicago, and then out to California. Anyway who has played Ticket to Ride probably recognizes many of these routes!\n\n\n\n\n\n\nGo further\n\n\n\nA fun exercise once you are comfortable with joins and map-based visualizatinos would be to join the railroad employment data to information about state population and geographic area, and also mapping information about railway routes, to get more insight into the factors driving railroad employment.\n\n\n\n\n\nThe FAOSTAT sheets are excerpts of the FAOSTAT database provided by the Food and Agriculture Association, an agency of the United Nations. We are using the file birds.csv that includes estimates of the stock of five different types of poultry (Chickens, Ducks, Geese and guinea fowls, Turkeys, and Pigeons/Others) for 248 areas for 58 years between 1961-2018. Estimated stocks are given in 1000 head.\nBecause we know (from challenge 1) that several of those areas include aggregated data (e.g., ) we are going to remove the aggregations, remove the unnecessary variables, and only work with the grouping variables available in the data. In a future challenge, we will join back on more data from the FAO to recreate regional groupings.\n\nbirds<-read_csv(\"_data/birds.csv\")%>%\n  select(-c(contains(\"Code\"), Element, Domain, Unit))%>%\n  filter(Flag!=\"A\")\nbirds\n\n\n\n  \n\n\n\n\nWhat is the average of Y for X groups?\nLets suppose we are starting off and know nothing about poultry stocks around the world, where could we start? Perhaps we could try to get a sense of the relative sizes of stocks of each of the five types of poultry, identified in the variable Item. Additionally, because some of the values may be missing, lets find out how many of the estimates are missing.\n\nbirds%>%\n  group_by(Item)%>%\n  summarise(avg_stocks = mean(Value, na.rm=TRUE),\n            med_stocks = median(Value, na.rm=TRUE),\n            n_missing = sum(is.na(Value)))\n\n\n\n  \n\n\n\nOn average, we can see that countries have far more chickens as livestock (\\(\\bar{x}\\)=58.4million head) than other livestock birds (average stocks range between 2 and 10 million head). However, the information from the median stock counts suggest that there is significant variation across countries along with a strong right hand skew with regards to chicken stocks. The median number of chickens in a country is 3.8 million head - significantly less than the mean of almost 60 million. Overall, missing data doesn’t seem to be a huge issue, so we will just use na.rm=TRUE and not worry too much about the missingness for now.\nIt could be that stock head counts have changed over time, so lets try selecting two points in time and seeing whether or not average livestock counts are changing.\n\n\n\n\n\n\npivot-wider\n\n\n\nIt can be difficult to visually report data in tidy format. For example, it is tough to compare two values when they are on different rows. In this example, I use pivot-wider to swap a tidy grouping variable into multiple columns to be more “table-like.” I then do some manual formatting to make it easy to compare the grouped estimates.\n\n\n\nt1<-birds%>%\n  filter(Year %in% c(1966, 2016))%>%\n  group_by(Year, Item)%>%\n  summarise(avg_stocks = mean(Value, na.rm=TRUE),\n            med_stocks = median(Value, na.rm=TRUE))%>%\n  pivot_wider(names_from = Year, values_from = c(avg_stocks, med_stocks))\n\nknitr::kable(t1,\n             digits=0,format.args = list(big.mark = \",\"),\n             col.names = c(\"Type\", \"1966\", \"2016\",\n                           \"1996\", \"2016\"))%>%\n  kableExtra::kable_styling(htmltable_class = \"lightable-minimal\")%>%\n  kableExtra::add_header_above(c(\" \" = 1, \"Mean Stock (1000)\" = 2,\n                                 \"Median Stock (1000)\" = 2))\n\n\n\n \n\n\nMean Stock (1000)\nMedian Stock (1000)\n\n  \n    Type \n    1966 \n    2016 \n    1996 \n    2016 \n  \n \n\n  \n    Chickens \n    25,264 \n    105,437 \n    2,315 \n    7,854 \n  \n  \n    Ducks \n    5,053 \n    14,842 \n    110 \n    236 \n  \n  \n    Geese and guinea fowls \n    2,468 \n    11,750 \n    97 \n    73 \n  \n  \n    Pigeons, other birds \n    3,314 \n    2,874 \n    3,000 \n    1,194 \n  \n  \n    Turkeys \n    843 \n    2,858 \n    74 \n    194 \n  \n\n\n\n\n\n\n\n\n\n\n\nkable and kableExtra\n\n\n\nI manually adjust table formatting (column names, setting significant digits, adding a comma) using kable and add a header row using kableExtra. Because df-print=paged option is set to make it easier to scroll through longer data frames, I need to directly specify that I want to produce an htmltable - not a default kable/rmarkdown table - when I use kable and kableExtra formatting directly.\n\n\nSure enough, it does look like stocks have changed significantly over time. The expansion of country-level chicken stocks over five decades between 1966 and 2016 are most noteworthy, with both average and median stock count going up by a factor of 4. Pigeons have never been very popular, and average stocks have actually decreased over the same time period while the other less popular bird - turkeys - saw significant incrases in stock count. Some countries increased specialization in goose and/or guinea fowl production, as the average stock count went up but the median went down over the same period.\n\n\n\n\n\n\nGo further\n\n\n\nIt could be really interesting to graph the rise and fall of poultry stocks overtime with these data, and match these changes to changes in population size and country GDP. Another option would be to match the countries back to regional groupings available from the UN FAO, a future “data join” challenge.\n\n\n\n\n\nThis data set contains 119,390 hotel bookings from two hotels (“City Hotel” and “Resort Hotel”) with an arrival date between July 2015 and August 2017 (more detail needed), including bookings that were later cancelled. Each row contains extensive information about a single booking:\n\nthe booking process (e.g., lead time, booking agent, deposit, changes made)\nbooking details (e.g., scheduled arrival date, length of stay)\nguest requests (e.g., type of room, meal(s) included, car parking)\nbooking channel (e.g., distribution, market segment, corporate affiliation for )\nguest information (e.g., child/adult, passport country)\nguest prior bookings (e.g., repeat customer, prior cancellations)\n\nThe data are a de-identified extract of real hotel demand data, made available by the authors.\n\nRead and make sense of the data\nThe hotel bookings data set is new to challenge 2, so we need to go through the same process we did during challenge 1 to find out more about the data. Lets read in the data and use the summmaryTools package to get an overview of the data set.\n\nbookings<-read_csv(\"_data/hotel_bookings.csv\")\n\nprint(summarytools::dfSummary(bookings,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\nData Frame Summary\nbookings\nDimensions: 119390 x 32\n  Duplicates: 31994\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      hotel\n[character]\n      1. City Hotel2. Resort Hotel\n      79330(66.4%)40060(33.6%)\n      \n      0\n(0.0%)\n    \n    \n      is_canceled\n[numeric]\n      Min  : 0Mean : 0.4Max  : 1\n      0:75166(63.0%)1:44224(37.0%)\n      \n      0\n(0.0%)\n    \n    \n      lead_time\n[numeric]\n      Mean (sd) : 104 (106.9)min ≤ med ≤ max:0 ≤ 69 ≤ 737IQR (CV) : 142 (1)\n      479 distinct values\n      \n      0\n(0.0%)\n    \n    \n      arrival_date_year\n[numeric]\n      Mean (sd) : 2016.2 (0.7)min ≤ med ≤ max:2015 ≤ 2016 ≤ 2017IQR (CV) : 1 (0)\n      2015:21996(18.4%)2016:56707(47.5%)2017:40687(34.1%)\n      \n      0\n(0.0%)\n    \n    \n      arrival_date_month\n[character]\n      1. August2. July3. May4. October5. April6. June7. September8. March9. February10. November[ 2 others ]\n      13877(11.6%)12661(10.6%)11791(9.9%)11160(9.3%)11089(9.3%)10939(9.2%)10508(8.8%)9794(8.2%)8068(6.8%)6794(5.7%)12709(10.6%)\n      \n      0\n(0.0%)\n    \n    \n      arrival_date_week_number\n[numeric]\n      Mean (sd) : 27.2 (13.6)min ≤ med ≤ max:1 ≤ 28 ≤ 53IQR (CV) : 22 (0.5)\n      53 distinct values\n      \n      0\n(0.0%)\n    \n    \n      arrival_date_day_of_month\n[numeric]\n      Mean (sd) : 15.8 (8.8)min ≤ med ≤ max:1 ≤ 16 ≤ 31IQR (CV) : 15 (0.6)\n      31 distinct values\n      \n      0\n(0.0%)\n    \n    \n      stays_in_weekend_nights\n[numeric]\n      Mean (sd) : 0.9 (1)min ≤ med ≤ max:0 ≤ 1 ≤ 19IQR (CV) : 2 (1.1)\n      17 distinct values\n      \n      0\n(0.0%)\n    \n    \n      stays_in_week_nights\n[numeric]\n      Mean (sd) : 2.5 (1.9)min ≤ med ≤ max:0 ≤ 2 ≤ 50IQR (CV) : 2 (0.8)\n      35 distinct values\n      \n      0\n(0.0%)\n    \n    \n      adults\n[numeric]\n      Mean (sd) : 1.9 (0.6)min ≤ med ≤ max:0 ≤ 2 ≤ 55IQR (CV) : 0 (0.3)\n      14 distinct values\n      \n      0\n(0.0%)\n    \n    \n      children\n[numeric]\n      Mean (sd) : 0.1 (0.4)min ≤ med ≤ max:0 ≤ 0 ≤ 10IQR (CV) : 0 (3.8)\n      0:110796(92.8%)1:4861(4.1%)2:3652(3.1%)3:76(0.1%)10:1(0.0%)\n      \n      4\n(0.0%)\n    \n    \n      babies\n[numeric]\n      Mean (sd) : 0 (0.1)min ≤ med ≤ max:0 ≤ 0 ≤ 10IQR (CV) : 0 (12.3)\n      0:118473(99.2%)1:900(0.8%)2:15(0.0%)9:1(0.0%)10:1(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      meal\n[character]\n      1. BB2. FB3. HB4. SC5. Undefined\n      92310(77.3%)798(0.7%)14463(12.1%)10650(8.9%)1169(1.0%)\n      \n      0\n(0.0%)\n    \n    \n      country\n[character]\n      1. PRT2. GBR3. FRA4. ESP5. DEU6. ITA7. IRL8. BEL9. BRA10. NLD[ 168 others ]\n      48590(40.7%)12129(10.2%)10415(8.7%)8568(7.2%)7287(6.1%)3766(3.2%)3375(2.8%)2342(2.0%)2224(1.9%)2104(1.8%)18590(15.6%)\n      \n      0\n(0.0%)\n    \n    \n      market_segment\n[character]\n      1. Aviation2. Complementary3. Corporate4. Direct5. Groups6. Offline TA/TO7. Online TA8. Undefined\n      237(0.2%)743(0.6%)5295(4.4%)12606(10.6%)19811(16.6%)24219(20.3%)56477(47.3%)2(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      distribution_channel\n[character]\n      1. Corporate2. Direct3. GDS4. TA/TO5. Undefined\n      6677(5.6%)14645(12.3%)193(0.2%)97870(82.0%)5(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      is_repeated_guest\n[numeric]\n      Min  : 0Mean : 0Max  : 1\n      0:115580(96.8%)1:3810(3.2%)\n      \n      0\n(0.0%)\n    \n    \n      previous_cancellations\n[numeric]\n      Mean (sd) : 0.1 (0.8)min ≤ med ≤ max:0 ≤ 0 ≤ 26IQR (CV) : 0 (9.7)\n      15 distinct values\n      \n      0\n(0.0%)\n    \n    \n      previous_bookings_not_canceled\n[numeric]\n      Mean (sd) : 0.1 (1.5)min ≤ med ≤ max:0 ≤ 0 ≤ 72IQR (CV) : 0 (10.9)\n      73 distinct values\n      \n      0\n(0.0%)\n    \n    \n      reserved_room_type\n[character]\n      1. A2. B3. C4. D5. E6. F7. G8. H9. L10. P\n      85994(72.0%)1118(0.9%)932(0.8%)19201(16.1%)6535(5.5%)2897(2.4%)2094(1.8%)601(0.5%)6(0.0%)12(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      assigned_room_type\n[character]\n      1. A2. D3. E4. F5. G6. C7. B8. H9. I10. K[ 2 others ]\n      74053(62.0%)25322(21.2%)7806(6.5%)3751(3.1%)2553(2.1%)2375(2.0%)2163(1.8%)712(0.6%)363(0.3%)279(0.2%)13(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      booking_changes\n[numeric]\n      Mean (sd) : 0.2 (0.7)min ≤ med ≤ max:0 ≤ 0 ≤ 21IQR (CV) : 0 (2.9)\n      21 distinct values\n      \n      0\n(0.0%)\n    \n    \n      deposit_type\n[character]\n      1. No Deposit2. Non Refund3. Refundable\n      104641(87.6%)14587(12.2%)162(0.1%)\n      \n      0\n(0.0%)\n    \n    \n      agent\n[character]\n      1. 92. NULL3. 2404. 15. 146. 77. 68. 2509. 24110. 28[ 324 others ]\n      31961(26.8%)16340(13.7%)13922(11.7%)7191(6.0%)3640(3.0%)3539(3.0%)3290(2.8%)2870(2.4%)1721(1.4%)1666(1.4%)33250(27.8%)\n      \n      0\n(0.0%)\n    \n    \n      company\n[character]\n      1. NULL2. 403. 2234. 675. 456. 1537. 1748. 2199. 28110. 154[ 343 others ]\n      112593(94.3%)927(0.8%)784(0.7%)267(0.2%)250(0.2%)215(0.2%)149(0.1%)141(0.1%)138(0.1%)133(0.1%)3793(3.2%)\n      \n      0\n(0.0%)\n    \n    \n      days_in_waiting_list\n[numeric]\n      Mean (sd) : 2.3 (17.6)min ≤ med ≤ max:0 ≤ 0 ≤ 391IQR (CV) : 0 (7.6)\n      128 distinct values\n      \n      0\n(0.0%)\n    \n    \n      customer_type\n[character]\n      1. Contract2. Group3. Transient4. Transient-Party\n      4076(3.4%)577(0.5%)89613(75.1%)25124(21.0%)\n      \n      0\n(0.0%)\n    \n    \n      adr\n[numeric]\n      Mean (sd) : 101.8 (50.5)min ≤ med ≤ max:-6.4 ≤ 94.6 ≤ 5400IQR (CV) : 56.7 (0.5)\n      8879 distinct values\n      \n      0\n(0.0%)\n    \n    \n      required_car_parking_spaces\n[numeric]\n      Mean (sd) : 0.1 (0.2)min ≤ med ≤ max:0 ≤ 0 ≤ 8IQR (CV) : 0 (3.9)\n      0:111974(93.8%)1:7383(6.2%)2:28(0.0%)3:3(0.0%)8:2(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      total_of_special_requests\n[numeric]\n      Mean (sd) : 0.6 (0.8)min ≤ med ≤ max:0 ≤ 0 ≤ 5IQR (CV) : 1 (1.4)\n      0:70318(58.9%)1:33226(27.8%)2:12969(10.9%)3:2497(2.1%)4:340(0.3%)5:40(0.0%)\n      \n      0\n(0.0%)\n    \n    \n      reservation_status\n[character]\n      1. Canceled2. Check-Out3. No-Show\n      43017(36.0%)75166(63.0%)1207(1.0%)\n      \n      0\n(0.0%)\n    \n    \n      reservation_status_date\n[Date]\n      min : 2014-10-17med : 2016-08-07max : 2017-09-14range : 2y 10m 28d\n      926 distinct values\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-23\n\n\n\nWow - there is a lot of information available here. Lets scan it and see what jumps out. First we can see that the summary function claims that there are almost 32,000 duplicates in the data. However, this is likely an artifact of the way that the bookings have been de-identified, and may reflect bookings with identical details but different individuals who made the bookings.\nWe can see that we are provided with limited information about the hotel. Hotels are identified only as “City” Hotel” or a “Resort Hotel”. Maybe we have bookings from only two hotels? Lets tentatively add that to our data description.\nThere is a flag for whether a booking is cancelled. This means that our universe of cases includes bookings where the guests showed up, as well as bookings that were later cancelled - we can add that to our data description.\nThere are multiple fields with the arrival date - year, month, etc. For now, we can tell that the arrival date of the bookings ranges between 2015 and 2017. More precise identification of the date range could be more easily done next challenge when we can recode the arrival date information using lubridate.But maybe it is possible to find out which values of month co-occur with specific years?\n\n\nWhich values of Y are nested within X?\nTo approach this question, we can narrow the dataset down to just the two variables of interest, and then use the distinct command.\n\nbookings%>%\n  select(arrival_date_year, arrival_date_month)%>%\n  distinct()\n\n\n\n  \n\n\n\nGreat - now we now that all bookings have arrival dates between June 2015 and August 2017, and can add that to the data description. Just for fun, lets see if we can confirm that the dates are the same for both hotels.\n\n\n\n\n\n\nslice()\n\n\n\nThis would be easier to investigate with proper date variables, but I am using slice to find the first and last row for each hotel, by position. This avoids printing out a long data list we have to scroll through, but would fail if the hotels had different sets of arrival month-year pairs.\n\n\n\nd<-bookings%>%\n  select(arrival_date_year, arrival_date_month)%>%\n  n_distinct\n\nbookings%>%\n  select(hotel, arrival_date_year, arrival_date_month)%>%\n  distinct()%>%\n  slice(c(1, d, d+1, d*2))\n\n\n\n  \n\n\n\nLets suppose we want to know whether or not the two hotels offer the same types of rooms? This is another query of the sort Which values of X are nested in y?\n\nbookings%>%\n  group_by(hotel)%>%\n  count(reserved_room_type)\n\n\n\n  \n\n\n\nIn this case, however, it is tough to directly compare - it appears that the hotel-roomtype pairs are not as consistent as the year-month pairs for the same hotels. A quick pivot-wider makes this comparison a little easier to visualize. Here we can see that the Resort Hotel has two additional room types: “H” and “L”.\n\nbookings%>%\n  group_by(hotel)%>%\n  count(reserved_room_type)%>%\n  pivot_wider(names_from= hotel, values_from = n)\n\n\n\n  \n\n\n\n\n\nWhat is the average of Y for group X?\nThe breakdown of rooms by hotel doesn’t shed much light on the room codes and what they might mean. Lets see if we can find average number of occupants and average price for each room type, and see if we can learn more about our data.\n\n\n\n\n\n\nmean(., na.rm=TRUE)\n\n\n\nI am using the mean function with the option na.rm=TRUE to deal with the four NA values in the children field, identified in the summary table above.\n\n\n\nt1<-bookings%>%\n  group_by(hotel, reserved_room_type)%>%\n  summarise(price = mean(adr),\n            adults = mean(adults),\n            children = mean(children+babies, na.rm=TRUE)\n            )%>%\n  pivot_wider(names_from= hotel, \n              values_from = c(price, adults, children))\n\nknitr::kable(t1,\n             digits=1,\n             col.names = c(\"Type\", \"City\", \"Resort\",\n                           \"City\", \"Resort\", \"City\", \"Resort\"))%>%\n  kableExtra::kable_styling(htmltable_class = \"lightable-minimal\")%>%\n  kableExtra::add_header_above(c(\"Room\" = 1, \"Price\" = 2,\n                                 \"Adults\" = 2, \"Children & Babies\" = 2))\n\n\nAverage Price and Occupancy, by hotel and room type\n \n\nRoom\nPrice\nAdults\nChildren & Babies\n\n  \n    Type \n    City \n    Resort \n    City \n    Resort \n    City \n    Resort \n  \n \n\n  \n    A \n    96.2 \n    76.2 \n    1.8 \n    1.8 \n    0.0 \n    0.0 \n  \n  \n    B \n    90.3 \n    104.7 \n    1.6 \n    2.0 \n    0.6 \n    0.0 \n  \n  \n    C \n    85.5 \n    161.4 \n    1.5 \n    2.0 \n    0.1 \n    1.4 \n  \n  \n    D \n    131.5 \n    103.6 \n    2.2 \n    2.0 \n    0.0 \n    0.1 \n  \n  \n    E \n    156.8 \n    114.5 \n    2.1 \n    2.0 \n    0.3 \n    0.0 \n  \n  \n    F \n    189.3 \n    132.8 \n    2.0 \n    2.0 \n    1.6 \n    0.1 \n  \n  \n    G \n    201.8 \n    168.2 \n    2.3 \n    2.0 \n    1.1 \n    1.4 \n  \n  \n    P \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n  \n  \n    H \n    NA \n    188.2 \n    NA \n    2.7 \n    NA \n    1.0 \n  \n  \n    L \n    NA \n    124.7 \n    NA \n    2.2 \n    NA \n    0.0 \n  \n\n\n\n\n\n\n\n\n\n\n\nkable & kableExtra\n\n\n\nI manually adjust table formatting (column names, plus adding a header row) using kable and kableExtra package, respectively. Also, because df-print=paged is the option set in the YAML header, I need to directly specify that I want to produce an htmltable - not a default kable/rmarkdown table.\n\n\nBased on these descriptives broken down by hotel and room type, we can speculate that the “H” and “L” room types at the resort are likely some sort of multi-bedroom suite (because the average number of adults is over 2.) Similarly, we can speculate that the difference between ABC and DEF may be something related to room size or quality (e.g., number and size of beds) and/or related to meals included with the rooms - but this would require further investigation to pin down!\n\n\n\n\n\n\nGo further\n\n\n\nThere is lots more to explore in the hotel bookings dataset, but it will be a lot easier once we recode the date fields using lubridate."
  },
  {
    "objectID": "posts/challenge8_solutions.html",
    "href": "posts/challenge8_solutions.html",
    "title": "Challenge 8 Solutions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE,\n                      message=FALSE)"
  },
  {
    "objectID": "posts/challenge8_solutions.html#challenge-overview",
    "href": "posts/challenge8_solutions.html#challenge-overview",
    "title": "Challenge 8 Solutions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\n\nread in multiple data sets, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nmutate variables as needed (including sanity checks)\njoin two or more data sets and analyze some aspect of the joined data\n\n(be sure to only include the category tags for the data you use!)"
  },
  {
    "objectID": "posts/challenge8_solutions.html#read-in-data",
    "href": "posts/challenge8_solutions.html#read-in-data",
    "title": "Challenge 8 Solutions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nmilitary marriages ⭐⭐\nfaostat ⭐⭐\nrailroads ⭐⭐⭐\nfed_rate ⭐⭐⭐\ndebt ⭐⭐⭐\nus_hh ⭐⭐⭐⭐\nsnl ⭐⭐⭐⭐⭐\n\n\nMilitary MarriagesFAOstat RegionsFAOstat: Regions\n\n\nThe excel workbook “ActiveDuty_MaritalStatus.xls” contains tabulations of total number of active duty military members classified by marital and family status, for each of the branches of military as well as the military overall. The sheet is a typical government table, so lets figure out what information is actually available! After that, we will clean a single sheet from the workbook, and use that effort to create a generic function that can be used to iterate through all sheets in the workbook to read them and then join them together into a single dataset.\n\nFind Information availableRead in single sheetCreate data reading functionPurrr to join\n\n\nLets first look at an example sheet from the workbook.\n\n\n\nTotal DOD Active Duty Marital Sheet\n\n\nWe can see a few things from this example sheet. First, we will need to skip 8 or 9 rows - the data first appears in row 10. Second, the tabular cells represent count values that capture the number of employees falling into subcategories created by 6 distinct grouping values: 1) Pay Grade Type: Enlisted/Officer/Warrant Officer 2) Pay Grade Level: 1-10 (fewer for non-Enlisted) 3) Marital status: Married/Single 4) Parent: Kids/noKids (Single only) 5) Spouse affiliation: Civilian/Military (Married only) 6) Gender: Male/Female\nOur goal is to recover cases that have these 6 (or really 5, if we collapse parent and spouse variables as we don’t have complete information) grouping variables to identify the case and the single value (count of active duty employees who fall into each of the resulting subcategories.)\nLooking back at the original excel sheet, we can see that we will need to not just skip the top rows, we will also need to delete several columns, and also rename variables in order to make it easy to separate out the three pieces of information contained in the column names. First, I create a vector with column names (to make it easier to reuse later when I pivot to tidy data) then I read in the data and inspect it to see if the columns worked as intended.\n\nmarital_cols <-c(\"d\", \"payGrade_payLevel\",\n            \"single_nokids_male\", \"single_nokids_female\", \"d\",\n            \"single_kids_male\", \"single_kids_female\", \"d\",\n            \"married_military_male\", \"married_military_female\", \"d\",\n            \"married_civilian_male\", \"married_civilian_female\", \"d\",\n            rep(\"d\", 3))\n\nread_excel(\"_data/ActiveDuty_MaritalStatus.xls\", \n           skip=8,\n           col_names = marital_cols\n           )\n\n\n\n  \n\n\n\nI can see that the variable names worked well, so this time I will read in the data and omit the original header row, and also filter out the various “TOTAL” rows that we don’t need to keep.\n\n\n\nmilitary<-read_excel(\"_data/ActiveDuty_MaritalStatus.xls\", \n           skip=9,\n           col_names = marital_cols\n           )%>%\n  select(!starts_with(\"d\"))%>%\n  filter(str_detect(payGrade_payLevel, \"TOTAL\", negate=TRUE))\n\nmilitary\n\n\n\n  \n\n\n\nIt looks like this worked well! Now we just need to pivot_longer with 3 columns, then separate out the information in the payGrade_payLevel variable and do a quick mutate to make paygrade easier to remember.\n\nmilitary_long <-military %>%\n  pivot_longer(cols = -1,\n               names_to = c(\"Marital\", \"Other\", \"Gender\"),\n               names_sep = \"_\",\n               values_to = \"count\")%>%\n  separate(payGrade_payLevel, \n           into = c(\"payGrade\", \"payLevel\"),\n           sep=\"-\")%>%\n  mutate(payGrade = case_when(\n    payGrade == \"E\" ~ \"Enlisted\",\n    payGrade == \"O\" ~ \"Officer\",\n    payGrade == \"W\" ~ \"Warrant Officer\"\n  ))\n\nmilitary_long\n\n\n\n  \n\n\n\nThis all looks like it works well. So now we will go on to creating a function with the steps, then applying it to multiple sheets.\n\n\nWe will call our new function read_military, and we will basically use the exact same commands as above. The big difference is that we will have a placeholder name (or argument) for the data sheet that will be passed to the new function.\nAnother difference is that when using read_excel() on a workbook with multiple sheets, we need to specify the sheetname of the sheet we wish to read in (we can also specify the sheet name). I then include the mutate() command to create a new column called branch, which comes from our sheet name.\nEverything else will be pretty identical to reading a single sheet - select(!starts_with(\"d\")) removes all columns that start with \"d\". We also filter out the word “Total” from payGrade_payLevel. pivot_longer()\n\nread_military<-function(sheet_name){\n  read_excel(\"_data/ActiveDuty_MaritalStatus.xls\", \n             sheet = sheet_name,\n             skip=9,\n             col_names = marital_cols\n             )%>%\n  mutate(\"branch\"=sheet_name) %>%\n  select(!starts_with(\"d\"))%>%\n  filter(str_detect(payGrade_payLevel, \"TOTAL\", negate=TRUE))%>%\n  pivot_longer(cols = contains(c(\"male\", \"female\")),\n               names_to = c(\"Marital\", \"Other\", \"Gender\"),\n               names_sep = \"_\",\n               values_to = \"count\")%>%\n  separate(payGrade_payLevel, \n           into = c(\"payGrade\", \"payLevel\"),\n           sep=\"-\")%>%\n  mutate(payGrade = case_when(\n    payGrade == \"E\" ~ \"Enlisted\",\n    payGrade == \"O\" ~ \"Officer\",\n    payGrade == \"W\" ~ \"Warrant Officer\"\n  ))\n}\n\n\n\nWe now have a function that is customized to read in the mmilitary active duty marital status sheets. We just need to use purrr - a package that is part of tidyverse but which may need to be installed and loaded on its own - to iterate through the list of sheets in the workbook.\n\nmilitary_sheets<-excel_sheets(\"_data/ActiveDuty_MaritalStatus.xls\")\n\nmilitary_sheets\n\n[1] \"TotalDoD\"    \"AirForce\"    \"MarineCorps\" \"Navy\"        \"Army\"       \n\n\nNow we have a list of sheet names to map with the function. Typically, a purrr::map function creates a list of data frames with each element in the original vector fed to the function as a single list element, as depicted below.\n\n\n\n\n\n\nPurrring Military Workbook\n\n\n\nWe are going to use map_dfr() to join the four elements of the list into a single dataframe. map is the generic purrr function, and adding on the dfr means that we want to turn all of the list elements created by map into a single datafame (df) that is joined by rows (r).\n\n\n\n\n\n\nusing map_dfr\n\n\n\nThere are functions in purrr that automatically convert the list created by map into the dataframe shape that you want, but you need to be careful that the objects being mapped have the correct column names (or rows, if you are using map_dfc() to avoid errors. Plain map is the safe option, but you then have to manually join the data.)\n\n\n\nmilitary_all <- map_dfr(\n  excel_sheets(\"_data/ActiveDuty_MaritalStatus.xls\")[2:5],\n  read_military)\nmilitary_all\n\n\n\n  \n\n\n\n\n\n\n\n\nThe FAOSTAT sheets are excerpts of the FAOSTAT database provided by the Food and Agriculture Association, an agency of the United Nations. There are two approaches we could use to joining these data. In this section, we are going to join livestock estimates from a single file to regional codes for the countries.\nThe file birds.csv that includes estimates of the stock of five different types of poultry (Chickens, Ducks, Geese and guinea fowls, Turkeys, and Pigeons/Others) in 248 areas for 58 years between 1961-2018. Because we know (from challenge 1) that several of those areas include aggregated data (e.g., ) we are going to remove the aggregations and replace the case level aggregations with the correct grouping variables from the file “FAOSTAT_country_groups.csv”, downloaded from Country Region definitions provided by the FAO. As a reminder, I am going to split the original birds data into country level and aggregated data, and then list the aggregated groups we found in Challenge 1.\n\nbirds_orig<-read_csv(\"_data/birds.csv\")%>%\n  select(-contains(\"Code\"))\n\nbirds_agg<-birds_orig%>%\n  filter(Flag==\"A\")\n\nbirds<-birds_orig%>%\n  filter(Flag!=\"A\")\n\nunique(birds_agg$Area)\n\n [1] \"World\"                     \"Africa\"                   \n [3] \"Eastern Africa\"            \"Middle Africa\"            \n [5] \"Northern Africa\"           \"Southern Africa\"          \n [7] \"Western Africa\"            \"Americas\"                 \n [9] \"Northern America\"          \"Central America\"          \n[11] \"Caribbean\"                 \"South America\"            \n[13] \"Asia\"                      \"Central Asia\"             \n[15] \"Eastern Asia\"              \"Southern Asia\"            \n[17] \"South-eastern Asia\"        \"Western Asia\"             \n[19] \"Europe\"                    \"Eastern Europe\"           \n[21] \"Northern Europe\"           \"Southern Europe\"          \n[23] \"Western Europe\"            \"Oceania\"                  \n[25] \"Australia and New Zealand\" \"Melanesia\"                \n[27] \"Micronesia\"                \"Polynesia\"                \n\n\nWith the FAO regional information, we can find out which countries are in which regions:\n\nfao_regions_orig <-read_csv(\"_data/FAOSTAT_country_groups.csv\")\nfao_regions<-fao_regions_orig%>%\n  select(`Country Group`, Country)%>%\n  rename(country_group = `Country Group`)%>%\n  distinct()\n\nfao_regions%>%\n  filter(country_group == \"Polynesia\")\n\n\n\n  \n\n\n\nClearly, some of the groups might overlap. For example, one group is called World and clearly overlaps with all other groups. So, before we join, lets remove the World group and then inspect the country-region pairings to ensure there are no other overlaps.\n\ntemp<-fao_regions%>%\n  group_by(country_group)%>%\n  summarize(n=n())%>%\n  arrange(desc(n))\nhalf <-c(1:round(nrow(temp)/2))\nknitr::kable(list(temp[half,],  \n           matrix(numeric(), nrow=0, ncol=1),\n           temp[-half,]), \n           caption = \"Countries in Country Groups\")%>%\n  kableExtra::kable_styling(font_size=12)\n\n\n\nCountries in Country Groups\n\n  \n    \n\n\n \n  \n    country_group \n    n \n  \n \n\n  \n    World \n    277 \n  \n  \n    Non-Annex I countries \n    161 \n  \n  \n    Net Food Importing Developing Countries \n    81 \n  \n  \n    Annex I countries \n    78 \n  \n  \n    High-income economies \n    64 \n  \n  \n    Africa \n    63 \n  \n  \n    Europe \n    63 \n  \n  \n    Americas \n    61 \n  \n  \n    Small Island Developing States \n    58 \n  \n  \n    Upper-middle-income economies \n    56 \n  \n  \n    Low Income Food Deficit Countries \n    55 \n  \n  \n    Asia \n    54 \n  \n  \n    Sub-Saharan Africa \n    53 \n  \n  \n    Latin America and the Caribbean \n    52 \n  \n  \n    Least Developed Countries \n    51 \n  \n  \n    Sub-Saharan Africa (including Sudan) \n    49 \n  \n  \n    Lower-middle-income economies \n    46 \n  \n  \n    Northern America and Europe \n    44 \n  \n  \n    OECD \n    36 \n  \n  \n    Oceania \n    35 \n  \n  \n    Low income economies \n    34 \n  \n  \n    Land Locked Developing Countries \n    32 \n  \n  \n    Caribbean \n    30 \n  \n  \n    European Union (27) \n    30 \n  \n  \n    Western Asia and Northern Africa \n    24 \n  \n  \n    Eastern Africa \n    23 \n  \n  \n    Eastern Asia and South-eastern Asia \n    20 \n  \n  \n    Southern Europe \n    20 \n  \n  \n    Western Asia \n    20 \n  \n\n\n\n \n    \n\n\n\n  \n\n  \n\n\n\n \n    \n\n\n \n  \n    country_group \n    n \n  \n \n\n  \n    Northern Europe \n    18 \n  \n  \n    Oceania excluding Australia and New Zealand \n    18 \n  \n  \n    Western Africa \n    18 \n  \n  \n    South America \n    17 \n  \n  \n    Central Asia and Southern Asia \n    14 \n  \n  \n    North and Central America \n    14 \n  \n  \n    Western Asia (exc. Armenia, Azerbaijan, Cyprus, Israel and Georgia) \n    13 \n  \n  \n    Eastern Europe \n    12 \n  \n  \n    Polynesia \n    12 \n  \n  \n    South-eastern Asia \n    11 \n  \n  \n    Western Europe \n    11 \n  \n  \n    Micronesia \n    10 \n  \n  \n    Eastern Asia \n    9 \n  \n  \n    Middle Africa \n    9 \n  \n  \n    Southern Asia \n    9 \n  \n  \n    Caucasus and Central Asia \n    8 \n  \n  \n    Central America \n    8 \n  \n  \n    Northern Africa \n    8 \n  \n  \n    Southern Asia (excluding India) \n    8 \n  \n  \n    Australia and New Zealand \n    6 \n  \n  \n    Eastern Asia (excluding Japan) \n    6 \n  \n  \n    Northern Africa (excluding Sudan) \n    6 \n  \n  \n    Northern America \n    6 \n  \n  \n    Central Asia \n    5 \n  \n  \n    Melanesia \n    5 \n  \n  \n    Southern Africa \n    5 \n  \n  \n    Eastern Asia (excluding China) \n    3 \n  \n  \n    Eastern Asia (excluding Japan and China) \n    3 \n  \n  \n    Antarctic Region \n    1 \n  \n\n\n\n \n  \n\n\n\n\n\nUnfortunately, we can see that many regions must overlap - which is super annoying. We need to essentially extract the country-level or regional groupings - of which there seem to be up to 7 or 8 - potentially nested or perhaps not - country grouping categories - to fully disentangle the various aggregations contained in the original data. The demonstration below quickly identifies four major grouping categories to extract and confirms that there are approximately 277 countries (or less) in each grouping category. More detailed work could be used to nest sub-regions within regions (e.g., Eastern Europe would nest within Europe, and so on.)\n\nfao_regions%>%\n  summarise(n=n())/277\n\n\n\n  \n\n\nfao_regions%>%\n  filter(str_detect(country_group, \"[aA]nnex\"))%>%\n  group_by(country_group)%>%\n  summarise(n=n())\n\n\n\n  \n\n\nfao_regions%>%\n  filter(str_detect(country_group, \"[aA]nnex\"))%>%\n  summarise(n=n())\n\n\n\n  \n\n\nfao_regions%>%\n  filter(str_detect(country_group, \"[iI]ncome\"))%>%\n  group_by(country_group)%>%\n  summarise(n=n())\n\n\n\n  \n\n\nfao_regions%>%\n  filter(str_detect(country_group, \"[iI]ncome\"))%>%\n  summarise(n=n())\n\n\n\n  \n\n\nfao_regions%>%\n  filter(str_detect(country_group, \"[Dd]evelop|OECD\"))%>%\n  group_by(country_group)%>%\n  summarise(n=n())\n\n\n\n  \n\n\nfao_regions%>%\n  filter(str_detect(country_group, \"[Dd]evelop|OECD\"))%>%\n  summarise(n=n())\n\n\n\n  \n\n\nmajor_regions<-c(\"Africa\", \"Asia\", \"Europe\", \"Americas\", \n                 \"Oceania\", \"Antarctic Region\")\n\nfao_regions%>%\n  filter(country_group %in% major_regions)%>%\n  summarise(n=n())\n\n\n\n  \n\n\n\nWe now use the unite command to create four new categorical variables corresponding to the four country groupings we have identified that contain most or all of the 277 countries included in the data.\n\nfao_regions_wide<-fao_regions%>%\n  filter(country_group!=\"World\")%>%\n  pivot_wider(names_from=country_group, values_from = 1)%>%\n  unite(\"gp_annex\", contains(\"Annex\"), \n        sep=\"\", na.rm=TRUE, remove=TRUE)%>%\n  unite(\"gp_major_region\", any_of(major_regions), \n        sep=\"\", na.rm=TRUE, remove=TRUE)%>%\n  unite(\"gp_income\", contains(\"Income\")|contains(\"income\"),\n        sep=\"\", na.rm=TRUE, remove=TRUE)%>%\n  unite(\"gp_develop\", contains(\"Develop\")|contains(\"OECD\"),\n        sep=\"\", na.rm=TRUE, remove=TRUE)%>%\n  select(Country, starts_with(\"gp\"))\n\n\nJoin to livestock data\nNow that we have countries and four regional breakdown indicators, we can join this data to our original livestock data by countryname. I am going to do a left_join, as each case includes a country, and we want to match countries to join the four regional indicators. Because the Country variable is described as Area in the original birds data, we want to explicitly set the two key fields for the join.\nWe should end up with the same number of rows in the “birds” data as we started off with. An inner_join would also work in this case, would effectively omit from the left-hand side data any aggregated cases left in by mistake (it would only keep rows where “Area” matched one of the “Country” values.)\n\nnrow(birds)\n\n[1] 13716\n\nbirds <- left_join(birds, fao_regions_wide,\n                   by = c(\"Area\" = \"Country\"))\n\nNow, you can summarize across countries grouped by income, development status, major region, or Annex/non-Annex.\n\n\n\nThe FAOSTAT sheets are excerpts of the FAOSTAT database provided by the Food and Agriculture Association, an agency of the United Nations. There are two approaches we could use to joining these data. In this section, we are going to join livestock estimates from multiple “faostat…” files.\nWe have already seen that the file birds.csv includes estimates of the stock of five different types of poultry (Chickens, Ducks, Geese and guinea fowls, Turkeys, and Pigeons/Others) in 248 areas for 58 years between 1961-2018. What about the other “FAOSTAT_*” files?\nLets read them into a single list, and see what we find. First, lets write a function to just read in the file and then print out the column names.\n\n\n\n\n\n\nlist.files()\n\n\n\nThere are base R functions that allow you to easily interact with the underlying system, just like you would at the command line or in the terminal. One of my favorites is list.files(), which does exactly what it says on the tin! The pattern matching used in the terminal may vary slightly depending on your operating system.\n\n\n\nread_faostat<-function(fn){\n  fao<-read_csv(str_c(\"_data\", fn, sep=\"/\"))%>%\n    select(-contains(\"Code\"))\n  \n  return(colnames(fao))\n}\n\nfao_files<-list.files(path=\"_data\", pattern=\"FAOSTAT*\")\nfao_files[2]<-\"birds.csv\"\n\nmap(fao_files, read_faostat)\n\n[[1]]\n[1] \"Domain\"           \"Area\"             \"Element\"          \"Item\"            \n[5] \"Year\"             \"Unit\"             \"Value\"            \"Flag\"            \n[9] \"Flag Description\"\n\n[[2]]\n[1] \"Domain\"           \"Area\"             \"Element\"          \"Item\"            \n[5] \"Year\"             \"Unit\"             \"Value\"            \"Flag\"            \n[9] \"Flag Description\"\n\n[[3]]\n[1] \"Domain\"           \"Area\"             \"Element\"          \"Item\"            \n[5] \"Year\"             \"Unit\"             \"Value\"            \"Flag\"            \n[9] \"Flag Description\"\n\n[[4]]\n[1] \"Domain\"           \"Area\"             \"Element\"          \"Item\"            \n[5] \"Year\"             \"Unit\"             \"Value\"            \"Flag\"            \n[9] \"Flag Description\"\n\n\nWe can see from this that the column names are identical in the four datasets, which makes them easy to merge. Additionally, we suspect from the birds dataset that the Domain variable will uniquely identify the dataset, lets see if we are correct by modifying our read_faostat function.\n\nread_faostat<-function(fn){\n  fao<-read_csv(str_c(\"_data\", fn, sep=\"/\"))%>%\n    select(-contains(\"Code\"))\n  \n  return(list(Domain = unique(fao$Domain),\n              Elements = unique(fao$Element),\n              Items = unique(fao$Item),\n              obs = nrow(fao)))\n}\n\nmap(fao_files, read_faostat)\n\n[[1]]\n[[1]]$Domain\n[1] \"Livestock Primary\"\n\n[[1]]$Elements\n[1] \"Milk Animals\" \"Yield\"        \"Production\"  \n\n[[1]]$Items\n[1] \"Milk, whole fresh cow\"\n\n[[1]]$obs\n[1] 36449\n\n\n[[2]]\n[[2]]$Domain\n[1] \"Live Animals\"\n\n[[2]]$Elements\n[1] \"Stocks\"\n\n[[2]]$Items\n[1] \"Chickens\"               \"Ducks\"                  \"Geese and guinea fowls\"\n[4] \"Turkeys\"                \"Pigeons, other birds\"  \n\n[[2]]$obs\n[1] 30977\n\n\n[[3]]\n[[3]]$Domain\n[1] \"Livestock Primary\"\n\n[[3]]$Elements\n[1] \"Laying\"     \"Yield\"      \"Production\"\n\n[[3]]$Items\n[1] \"Eggs, hen, in shell\"\n\n[[3]]$obs\n[1] 38170\n\n\n[[4]]\n[[4]]$Domain\n[1] \"Live Animals\"\n\n[[4]]$Elements\n[1] \"Stocks\"\n\n[[4]]$Items\n[1] \"Asses\"     \"Camels\"    \"Cattle\"    \"Goats\"     \"Horses\"    \"Mules\"    \n[7] \"Sheep\"     \"Buffaloes\" \"Pigs\"     \n\n[[4]]$obs\n[1] 82116\n\n\nWhat an easy way to learn a lot about the data quickly. Sure enough, we can now merge the data back together and have way too much information in the same R object :-) This time, we will modify the read_faostat function to only read in the data, and then use purrr::map_dfr() to join.\n\nread_faostat<-function(fn){\n  fao<-read_csv(str_c(\"_data\", fn, sep=\"/\"))%>%\n    select(-contains(\"Code\"))\n  return(fao)\n}\n\nfaostat<- map_dfr(fao_files, read_faostat)%>%\n  inner_join(fao_regions_wide, \n             by=c(\"Area\" = \"Country\"))\n\nhead(faostat)"
  },
  {
    "objectID": "posts/challenge2_instructions.html",
    "href": "posts/challenge2_instructions.html",
    "title": "Challenge 2 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_instructions.html#challenge-overview",
    "href": "posts/challenge2_instructions.html#challenge-overview",
    "title": "Challenge 2 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_instructions.html#read-in-the-data",
    "href": "posts/challenge2_instructions.html#read-in-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, available in the posts/_data folder, using the correct R package and command.\n\nrailroad*.csv or StateCounty2012.xlsx ⭐\nFAOstat*.csv ⭐⭐⭐\nhotel_bookings ⭐⭐⭐⭐\n\n\n\n\nAdd any comments or documentation as needed. More challenging data may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge2_instructions.html#describe-the-data",
    "href": "posts/challenge2_instructions.html#describe-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data)."
  },
  {
    "objectID": "posts/challenge2_instructions.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_instructions.html#provide-grouped-summary-statistics",
    "title": "Challenge 2 Instructions",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\n\n\n\n\nExplain and Interpret\nBe sure to explain why you choose a specific group. Comment on the interpretation of any interesting differences between groups that you uncover. This section can be integrated with the exploratory data analysis, just be sure it is included."
  },
  {
    "objectID": "posts/challenge6_instructions.html",
    "href": "posts/challenge6_instructions.html",
    "title": "Challenge 6 Instructions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge6_instructions.html#challenge-overview",
    "href": "posts/challenge6_instructions.html#challenge-overview",
    "title": "Challenge 6 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nmutate variables as needed (including sanity checks)\ncreate at least one graph including time (evolution)\n\n\ntry to make them “publication” ready (optional)\nExplain why you choose the specific graph type\n\n\nCreate at least one graph depicting part-whole or flow relationships\n\n\ntry to make them “publication” ready (optional)\nExplain why you choose the specific graph type\n\nR Graph Gallery is a good starting point for thinking about what information is conveyed in standard graph types, and includes example R code.\n(be sure to only include the category tags for the data you use!)"
  },
  {
    "objectID": "posts/challenge6_instructions.html#read-in-data",
    "href": "posts/challenge6_instructions.html#read-in-data",
    "title": "Challenge 6 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\ndebt ⭐\nfed_rate ⭐⭐\nabc_poll ⭐⭐⭐\nusa_hh ⭐⭐⭐\nhotel_bookings ⭐⭐⭐⭐\nair_bnb ⭐⭐⭐⭐⭐\n\n\n\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge6_instructions.html#tidy-data-as-needed",
    "href": "posts/challenge6_instructions.html#tidy-data-as-needed",
    "title": "Challenge 6 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\n\n\n\nAre there any variables that require mutation to be usable in your analysis stream? For example, do you need to calculate new values in order to graph them? Can string values be represented numerically? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here."
  },
  {
    "objectID": "posts/challenge6_instructions.html#time-dependent-visualization",
    "href": "posts/challenge6_instructions.html#time-dependent-visualization",
    "title": "Challenge 6 Instructions",
    "section": "Time Dependent Visualization",
    "text": "Time Dependent Visualization"
  },
  {
    "objectID": "posts/challenge6_instructions.html#visualizing-part-whole-relationships",
    "href": "posts/challenge6_instructions.html#visualizing-part-whole-relationships",
    "title": "Challenge 6 Instructions",
    "section": "Visualizing Part-Whole Relationships",
    "text": "Visualizing Part-Whole Relationships"
  },
  {
    "objectID": "posts/challenge1_instructions.html",
    "href": "posts/challenge1_instructions.html",
    "title": "Challenge 1 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_instructions.html#challenge-overview",
    "href": "posts/challenge1_instructions.html#challenge-overview",
    "title": "Challenge 1 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_instructions.html#read-in-the-data",
    "href": "posts/challenge1_instructions.html#read-in-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_instructions.html#describe-the-data",
    "href": "posts/challenge1_instructions.html#describe-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data)."
  },
  {
    "objectID": "posts/challenge5_instructions.html",
    "href": "posts/challenge5_instructions.html",
    "title": "Challenge 5 Instructions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge5_instructions.html#challenge-overview",
    "href": "posts/challenge5_instructions.html#challenge-overview",
    "title": "Challenge 5 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nmutate variables as needed (including sanity checks)\ncreate at least two univariate visualizations\n\n\ntry to make them “publication” ready\nExplain why you choose the specific graph type\n\n\nCreate at least one bivariate visualization\n\n\ntry to make them “publication” ready\nExplain why you choose the specific graph type\n\nR Graph Gallery is a good starting point for thinking about what information is conveyed in standard graph types, and includes example R code.\n(be sure to only include the category tags for the data you use!)"
  },
  {
    "objectID": "posts/challenge5_instructions.html#read-in-data",
    "href": "posts/challenge5_instructions.html#read-in-data",
    "title": "Challenge 5 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\ncereal ⭐\npathogen cost ⭐\nAustralian Marriage ⭐⭐\nAB_NYC_2019.csv ⭐⭐⭐\nrailroads ⭐⭐⭐\nPublic School Characteristics ⭐⭐⭐⭐\nUSA Households ⭐⭐⭐⭐⭐\n\n\n\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge5_instructions.html#tidy-data-as-needed",
    "href": "posts/challenge5_instructions.html#tidy-data-as-needed",
    "title": "Challenge 5 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\n\n\n\nAre there any variables that require mutation to be usable in your analysis stream? For example, do you need to calculate new values in order to graph them? Can string values be represented numerically? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here."
  },
  {
    "objectID": "posts/challenge5_instructions.html#univariate-visualizations",
    "href": "posts/challenge5_instructions.html#univariate-visualizations",
    "title": "Challenge 5 Instructions",
    "section": "Univariate Visualizations",
    "text": "Univariate Visualizations"
  },
  {
    "objectID": "posts/challenge5_instructions.html#bivariate-visualizations",
    "href": "posts/challenge5_instructions.html#bivariate-visualizations",
    "title": "Challenge 5 Instructions",
    "section": "Bivariate Visualization(s)",
    "text": "Bivariate Visualization(s)\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge3_solutions.html",
    "href": "posts/challenge3_solutions.html",
    "title": "Challenge 3 Solutions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE,\n                      message=FALSE, cache=TRUE)"
  },
  {
    "objectID": "posts/challenge3_solutions.html#challenge-overview",
    "href": "posts/challenge3_solutions.html#challenge-overview",
    "title": "Challenge 3 Solutions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nanticipate the shape of pivoted data, and\npivot the data into tidy format using pivot_longer\n\n\nExampleAnimal Weights ⭐Eggs ⭐⭐/⭐⭐⭐Australian Marriage Ballot ⭐⭐⭐USA Households ⭐⭐⭐⭐\n\n\nThe first step in pivoting the data is to try to come up with a concrete vision of what the end product should look like - that way you will know whether or not your pivoting was successful.\nOne easy way to do this is to think about the dimensions of your current data (tibble, dataframe, or matrix), and then calculate what the dimensions of the pivoted data should be.\nSuppose you have a dataset with \\(n\\) rows and \\(k\\) variables. In our example, 3 of the variables are used to identify a case, so you will be pivoting \\(k-3\\) variables into a longer format where the \\(k-3\\) variable names will move into the names_to variable and the current values in each of those columns will move into the values_to variable. Therefore, we would expect \\(n * (k-3)\\) rows in the pivoted dataframe!\nSuppose you have a dataset with \\(n\\) rows and \\(k\\) variables. In our example, 3 of the variables are used to identify a case, so you will be pivoting \\(k-3\\) variables into a longer format where the \\(k-3\\) variable names will move into the names_to variable and the current values in each of those columns will move into the values_to variable. Therefore, we would expect \\(n * (k-3)\\) rows in the pivoted dataframe!\n\nFind current and future data dimensions\nLets see if this works with a simple example.\n\ndf<-tibble(country = rep(c(\"Mexico\", \"USA\", \"France\"),2),\n           year = rep(c(1980,1990), 3), \n           trade = rep(c(\"NAFTA\", \"NAFTA\", \"EU\"),2),\n           outgoing = rnorm(6, mean=1000, sd=500),\n           incoming = rlogis(6, location=1000, \n                             scale = 400))\ndf\n\n\n Example\n  \n\n\n#existing rows/cases\nnrow(df)\n\n[1] 6\n\n#existing columns/cases\nncol(df)\n\n[1] 5\n\n#expected rows/cases\nnrow(df) * (ncol(df)-3)\n\n[1] 12\n\n# expected columns \n3 + 2\n\n[1] 5\n\n\nOr simple example has \\(n = 6\\) rows and \\(k - 3 = 2\\) variables being pivoted, so we expect a new dataframe to have \\(n * 2 = 12\\) rows x \\(3 + 2 = 5\\) columns.\n\n\nPivot the data\n\ndf<-pivot_longer(df, col = c(outgoing, incoming),\n                 names_to=\"trade_direction\",\n                 values_to = \"trade_value\")\ndf\n\n\n Pivoted Example\n  \n\n\n\nYes, once it is pivoted long, our resulting data are \\(12x5\\) - exactly what we expected!\n\n\n\nThe animal weights dataset contains tabular-style data, with cells representing the average live animal weight (in kg) of 16 types of livestock for each of 9 geographic areas as defined by the Intergovernmental Panel on Climate Change (IPCC. Livestock weights are a critical part of the Global Livestock Envrionmental Assessment Model used by the FAO.\n\nanimal_weight<-read_csv(\"_data/animal_weight.csv\")\nanimal_weight\n\n\n\n  \n\n\n\nBecause the animal weights data is in tabular format, it is easy to see that \\(n=9\\) regions (categories or cases) in the original data, and that there are \\(k=16\\) types of livestock (categories or columns). Therefore, we expect the pivoted dataset to have \\(9 * 16\\) = 144 rows and 3 columns (region, animal type, and animal weight.)\n\n\n\n\n\n\ninline R code\n\n\n\nIf you check out the code above, you will see that I didn’t use a calculator to figure out \\(9*16=144\\), but used inline r code like this: 144.\n\n\n\nPivot the data\n\nanimal_weight_longer<-pivot_longer(animal_weight, \n                                    col=-`IPCC Area`,\n                                    names_to = \"Livestock\",\n                                    values_to = \"Weight\")\nanimal_weight_longer\n\n\n\n  \n\n\n\nYes, it looks like we ended up with 144 rows and 3 columns, exactly as expected!\n#Go further\nstringr functions, and separate from tidyr, would be useful in helping split out additional infromation from the Livestock column.\n\n\n\nThis section covers pivoting for the organic eggs data, available in both excel and (partially cleaned) .csv format. The data reports the average price per carton paid to the farmer or producer for organic eggs (and organic chicken), reported monthly from 2004 to 20013. Average price is reported by carton type, which can vary in both size (x-large or large) and quantity (half-dozen or dozen.)\nIf you are using the eggs_tidy.csv, you can skip the first section as your data is in .csv format and already partially cleaned. The first section reviews data read-in and cleaning for the organicpoultry.xls file.\n\nRead and Clean the dataPivot Type OnlyPivot Size and Quantity\n\n\nThere are three sheets in the organicpoultry.xls workbook: one titled Data, one titled “Organic egg prices, 2004-13” and one with a similar title for chicken prices. While I can tell all of this from inspection, I can also use a ask R to return the sheet names for me.\n\n\n\n\n\n\nGet sheet names with excel_sheets()\n\n\n\nBoth readxl and googlesheets4 have a function that can return sheet names as a vector. This is really useful if you need to parse and read multiple sheets in the same workbook.\n\n\n\nexcel_sheets(\"_data/organiceggpoultry.xls\")\n\n[1] \"Data\"                            \"Organic egg prices, 2004-13\"    \n[3] \"Organic poultry prices, 2004-13\"\n\n\nWhile it may seem like it would be easier to read in the individual egg prices and chicken prices, the amount of formatting introduced into the second and third sheets is pretty intimidating (see the screenshot below.) There are repeated headers to remove, a year column to shift, and other formatting issues. Ironically, it may be easier to read in the egg data from the Data sheet, with a skip of 5 (to skip the table title, etc), custom column names designed for pivoting to two categories (final section) and only reading in columns B to F.\n\n\n\nOrganic Poultry Data\n\n\n\n\n\nOrganic Poultry Egg Prices\n\n\n\n\n\n\n\n\nHard-coding Table Formats\n\n\n\nFormatted excel tables are a horrible data source, but may be the only way to get some data. If table formatting is consistent from year to year, hard-coding can be an acceptable approach. If table format is inconsistent, then more powerful tools are needed.\n\n\n\neggs_orig<-read_excel(\"_data/organiceggpoultry.xls\",\n                      sheet=\"Data\",\n                      range =cell_limits(c(6,2),c(NA,6)),\n                      col_names = c(\"date\", \"xlarge_dozen\",\n                               \"xlarge_halfdozen\", \"large_dozen\",\n                               \"large_halfdozen\")\n                 )\neggs_orig\n\n\n\n  \n\n\n\nSometimes there are notes in the first column of tables, so lets make sure that isn’t an issue.\n\neggs_orig%>%\n  count(date)\n\n\n\n  \n\n\n\nWe need to remove the “note” indicators in two of the rows. Some characters require an escape to be included in regular expressions, but this time it is straightforward to find ” /1”.\n\neggs<-eggs_orig%>%\n  mutate(date = str_remove(date, \" /1\"))\n\nOne final step is needed to split the year variable away from the month. You will often need to separate out two variables from a single column when working with published tables, and also need to use the equivalent of dragging to fill in a normal spreadsheet. Lets look at the easiest way to fix both of these issues.\n\n\n\n\n\n\ntidyr::separate()\n\n\n\nSeparate is a fantastic function for working with strings. It will break a string column into multiple new (named) columns, at the indicated separator character (e.g., “,” or ” “). The old variable is automatically removed, but can be left.\n\n\n\n\n\n\n\n\ntidyr::fill()\n\n\n\nFill works like dragging to fill functionality in a spreadsheet. You can choose the direction to fill.\n\n\n\neggs<-eggs%>%\n  separate(date, into=c(\"month\", \"year\"), sep=\" \")%>%\n  fill(year)\neggs\n\n\n\n  \n\n\n\n\n\nLooking at the data, we can see that each of the original 120 cases consist of a year-month combination (e.g., January 2004), while the values are the average price (in cents) of four different types of eggs (e.g., large_half_dozen, large_dozen, etc) So to tidy our data, we should create a matrix with a year-month-eggType combination, with a single price value for each case.\nTo do this (and make our data easier to graph and analyze), we can pivot longer - changing our data from 120 rows with 6 variables (2 grouping and 4 values) to 480 rows of 4 variables (with 3 grouping variables and a single price value).\n\neggs_long<-eggs%>%\n  pivot_longer(cols=contains(\"large\"), \n               names_to = \"eggType\",\n               values_to = \"avgPrice\"\n  )\neggs_long\n\n\n\n  \n\n\n\nWell, that was super easy. But wait, what if you are interested in egg size - you want to know how much more expensive extra-large eggs are compared to large eggs. Right now, that will be annoying, as you will have to keep sorting out the egg quantity - whether the price is for a half_dozen or a dozen eggs.\n\n\nWouldn’t it be nice if we had two new columns - size and quantity - in place of the existing eggType categorical variable? In other words, to have fully tidy data, we would need 4 grouping variables (year, month, size, and quantity) and the same value (price). So, we want to use pivot longer, but we will be adding two new category variables (for a total of 4) and this will cut the number of rows in half (to 240).\nHow can we let R know what we want it to do?? Thankfully, we created pretty systematic column names for egg types in our original data, following the general pattern: size-quantity. Maybe we can use this to our advantage? Working with patterns in the names_sep option of the pivot functions makes it easier than you would think to pivot four existing columns into two new columns.\n\neggs_long<- eggs%>%\n  pivot_longer(cols=contains(\"large\"),\n               names_to = c(\"size\", \"quantity\"),\n               names_sep=\"_\",\n               values_to = \"price\"\n  )\neggs_long\n\n\n\n  \n\n\n\n\n\n\n\n\nThis is another tabular data source published by the Australian Bureau of Statistics that requires a decent amount of cleaning. In 2017, Australia conducted a postal survey to gauge citizens’ opinions towards same sex marriage: “Should the law be changed to allow same-sex couples to marry?” All Australian citizens are required to vote in elections, so citizens could respond in one of four ways: vote yes, vote no, vote in an unclear way, or fail to vote. (See the “Explanatory Notes” sheet for more details.)\nThe provided table includes estimates of the proportion of citizens choosing each of the four options, aggregated by Federal Electoral District, which are nested within one of 8 overarching Electoral Divisions. Here is a quick image showing the original table format.\n\n\n\nAustralian Marriage Data\n\n\n\nIdentify desired data structure\nInspection reveals several critical issues to address: - Typical long header (skip = 7) - No single row with variable names - Two redundant values (count and percentage - percentage is easy to recover from complete count data) - Total columns that are redundant (remove) - The sum of “Yes” and “No” votes appears to be redundant with Response Clear in columns I and J - District and Division are in the same column\nIn this example, we are going to identify the desired structure early in the process, because clever naming of variables makes it much easier to use pivot functions. We will skip reading in redundant data (proportions and “totals” columns), and then can identify four potentially distinct pieces of information. Three grouping variables: Division (in column 1), District (also in column 1), and citizen Response (yes, no, unclear, and non-response), plus one value: aggregated response Count.\nOur basic data reading and cleaning process should therefore follow these steps:\n\nRead in data, skipping unneeded columns and renaming variables\nCreate Division and District variables using separate() and fill()\npivot_longer() four response variables into 2 new Response and Count variables (double the number of rows)\n\n\nRead DataSeparate District and DivisionPivot_longer\n\n\nIt is best to confine serious hard-coding to the initial data read in step, to make it easy to locate and make changes or replicate in the future. So, we will use a combination of tools introduced earlier to read and reformat the data: skip and col_names to read in the data, select to get rid of unneeded columns, and filter to get rid of unneeded rows. We also use the drop_na function to filter unwanted rows.\n\nvote_orig <- read_excel(\"_data/australian_marriage_law_postal_survey_2017_-_response_final.xls\",\n           sheet=\"Table 2\",\n           skip=7,\n           col_names = c(\"District\", \"Yes\", \"del\", \"No\", rep(\"del\", 6), \"Illegible\", \"del\", \"No Response\", rep(\"del\", 3)))%>%\n  select(!starts_with(\"del\"))%>%\n  drop_na(District)%>%\n  filter(!str_detect(District, \"(Total)\"))%>%\n  filter(!str_starts(District, \"\\\\(\"))\nvote_orig\n\n\n\n  \n\n\n\n\n\nThe most glaring remaining issue is that the administrative Division is not in its own column, but is on its own row within the District column. The following code uses case_when to make a new Division variable with an entry (e.g., New South Wales Division) where there is a Division name in the District column, and otherwise it create just an empty space. After that, fill can be used to fill in empty spaces with the most recent Division name. We then filter out rows with only the title information.\n\nvote<- vote_orig%>%\n  mutate(Division = case_when(\n    str_ends(District, \"Divisions\") ~ District,\n    TRUE ~ NA_character_ ))%>%\n  fill(Division, .direction = \"down\")\nvote<- filter(vote,!str_detect(District, \"Division|Australia\"))\nvote\n\n\n\n  \n\n\n\n\n\nSupposed we wanted to create a stacked bar chart to compare the % who votes Yes to the people who either said No or didn’t vote. Or if we wanted to use division level characteristics to predict the proortion of people voting in a specific way? In both cases, we would need tidy data, which requires us to pivot longer into the original (aggregated) data format: Division, District, Response, Count. We should end up with 600 rows and 4 columns.\n\nvote_long<- vote%>%\n  pivot_longer(\n    cols = Yes:`No Response`,\n    names_to = \"Response\",\n    values_to = \"Count\"\n  )\nvote\n\n\n\n  \n\n\n\n\n\n\n\n\n\nThe excel workbook “USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019” is clearly a table of census-type household data (e.g., Current Population Study or CPS, American Community Study or ACS, etc.) Row 3 of the workbook provides a link to more details about the origin of the data used to produce the table.\nThe cases in this example are essentially year-identity groups, where I use the term identity to refer to the wide range of ways that the census can cluster racial and identity identity. While there are 12 categories in the data, many of these overlap and/or are not available in specific years. For example, one category is “All Races”, and it overlaps with all other categories but cannot be easily eliminated because it isn’t clear how\n\n\n\nExcel Workbook Screenshot\n\n\n\nIdentify desired data structure\nInspection of the excel workbook reveals several critical features of the data.\n\ncolumn names (of a sort) are in rows 4 and 5 (skip=5 and rename)\nfirst column includes year and race/hispanic origin households\nfirst column appears to have notes of some sort (remove notes)\nthere are end notes starting in row 358 (n_max = 352)\n“Total” column appears to be redundant proportion info\n\nThe data appears to have two grouping variables (year and identity), plus several values:\n\na count of number of households\nmedian and mean income (and associated margin of error)\nproportion of households with hhold income in one of 9 designated ranges or brackets\n\nThe final data should potentially be split into two data tables - one with averages for a group of households, the other with proportions information based on income brackets. There is less clarity about how to handle the potential overlap between race and hispanic group identity information.\n\nRead dataClean and separate yearSanity check identitySplit the data\n\n\nTo make it easy to pivot, we will read in income brackets cleanly, along with clear variable names for the estimated average (and standard error) of hhold income for each designated identity group.\n\nincome_brackets <- c(i1 = \"Under $15,000\",\n                     i2 = \"$15,000 to $24,999\",\n                     i3 = \"$25,000 to $34,999\",\n                     i4= \"$35,000 to $49,999\",\n                     i5 = \"$50,000 to $74,999\",\n                     i6 = \"$75,000 to $99,999\",\n                     i7 = \"$100,000 to $149,999\",\n                     i8 = \"$150,000 to $199,999\",\n                     i9 = \"$200,000 and over\")\n\nushh_orig <- read_excel(\"_data/USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019.xlsx\",\n         skip=5,\n         n_max = 352,\n         col_names = c(\"year\", \"hholds\", \"del\",\n                       str_c(\"income\",1:9,sep=\"_i\"),\n                       \"median_inc\", \"median_se\", \n                       \"mean_inc\",\"mean_se\"))%>%\n  select(-del)\n  ushh_orig \n\n\n\n  \n\n\n\n\n\nThe current year column has both year and identity information on the hholds, as well as notes that need to be removed. Because identity labels have spaces, we will need to remove those first before our typical approach to removing notes using separate is going to work.\n\n\n\n\n\n\nRegex and Regexr\n\n\n\nRegular expressions are a critical tool for messy, real world data where you will need to search, replace, and extract information from string variables. Learning regex is tough, but Regexer makes it much easier!\n\n\n\nushh_orig%>%\n  filter(str_detect(year, \"[[:alpha:]]\"))\n\n\n\n  \n\n\n\nNow that we know how to use regular expressions to find the household identity information, we can quickly separate out the identity information from the years, then do the standard fill prior to removing the unneeded category rows.\nOnce that is done, we can use separate to remove the notes from the year column. Removing notes from the identity column is a bit trickier, and requires regex to find cases where there is a space then at least one numeric digit. The notes are important for helping orient yourself to the data, and if I were taking my time, I would document the relevant notes in the text at this point, to remind myself if I need to remember details about how the categories were created and/or why they vary over time.\nNote that several variables are read in as non-numeric, so I’m fixing them in a single statement!\n\nushh_id<-ushh_orig%>%\n  mutate(identity = case_when(\n    str_detect(year, \"[[:alpha:]]\") ~ year,\n    TRUE ~ NA_character_\n  ))%>%\n  fill(identity)%>%\n  filter(!str_detect(year, \"[[:alpha:]]\"))\n\nushh_id<-ushh_id%>%\n  separate(year, into=c(\"year\", \"delete\"), sep=\" \")%>%\n  mutate(identity = str_remove(identity, \" [0-9]+\"),\n         across(any_of(c(\"hholds\", \"mean_inc\", \"mean_se\", \"year\")), \n                parse_number))%>%\n  select(-delete)\n\nushh_id\n\n\n\n  \n\n\n\n\n\nEven from the detailed notes, it is difficult to fully understand what is going on with the identity variable, and whether all of the values are available in every year. A simple sanity check is to pick out several years mentioned in the notes and see if the number of households are available for all categories, and also check to see if there are specific categories that add up to the “all races” category.\n\nushh_id%>%\n  filter(year%in%c(1970, 1972, 1980, 2001, 2002))%>%\n  select(identity, hholds, year)%>%\n  pivot_wider(values_from=hholds, names_from=year)\n\n\n\n  \n\n\n\nBased on these examples, we can now confirm that the survey did not include a question about Hispanic background prior to 197228, that only “White” and “Black” (and not “Asian”) were systematically recorded prior to 2002, and that other mentioned dates of changes are not relevant to the categories represented in the data. Additionally, we can see from the example years that it would be reasonable to create a consistent time series that collapses the “White” and “White Alone” and “Black” and “Black Alone labels.\nWhile it might appear plausible to create distinct variables for race and hispanic, there is an instructive note in the worksheet that suggests this is probably not possible:\n\nBecause Hispanics may be any race, data in this report for Hispanics overlap with data for racial groups. Hispanic origin was reported by 15.6 percent of White householders who reported only one race, 5.0 percent of Black householders who reported only one race, and 2.5 percent of Asian householders who reported only one race. Data users should exercise caution when interpreting aggregate results for the Hispanic population and for race groups because these populations consist of many distinct groups that differ in socioeconomic characteristics, culture, and recency of immigration. Data were first collected for Hispanics in 1972.\n\n— Note 28\n\n\nIn fact, when we try to get the various hhouse number estimates to add up to the TOTAL households, it is nearly impossible to do so. For now, the most expeditious and likely acceptable approach may be to simply keep a reasonable set of categories that minimizes category overlaps.\n\n\n\n\n\n\nKeep your original data\n\n\n\nOriginal data that has been carefully documented can be overly detailed and broken into categories that make systematic analysis difficult. When you simplify data categories for exploratory work, keep the original data so that you can reintroduce it at the appropriate point.\n\n\n\nushh <-ushh_id%>%\n  mutate(gp_identity = case_when(\n   identity %in% c(\"BLACK\", \"BLACK ALONE\") ~ \"gp_black\",\n    identity %in% c(\"ASIAN ALONE OR IN COMBINATION\",\n                  \"ASIAN AND PACIFIC ISLANDER\") ~ \"gp_asian\",\n    identity %in% c(\"WHITE, NOT HISPANIC\", \n                    \"WHITE ALONE, NOT HISPANIC\") ~ \"gp_white\",\n    identity %in% c(\"HISPANIC (ANY RACE)\") ~ \"gp_hisp\",\n    identity %in% c(\"ALL RACES\") ~ \"gp_all\"\n  ))%>%\n  filter(!is.na(gp_identity))%>%\n  group_by(year, gp_identity)%>%\n  summarise(across(c(starts_with(\"inc\"),starts_with(\"me\"),\n                     \"hholds\"), \n                   ~sum(.x, na.rm=TRUE)))%>%\n  ungroup()\n\nushh\n\n\n\n  \n\n\n\n\nushh%>%\n  filter(year%in%c(1972, 2001, 2002))%>%\n  select(gp_identity, hholds, year)%>%\n  pivot_wider(values_from=hholds, names_from=year)\n\n\n\n  \n\n\n\n\n\nWe have identified reasonable groups where the parts approximately add up to the whole. They are not perfect, but not horrible given the incomplete and overlapping categories. To get much better, we would need to work with the original ACS or IPUMS microdata.\nNow, there may be times we need to split the data - for example, if we want to graph the income brackets as part of the total households. Lets see if we can split and pivot the data.\n\nushh_brackets <-ushh%>%\n  ungroup()%>%\n  select(year, gp_identity, hholds, starts_with(\"income\"))%>%\n  pivot_longer(cols=starts_with(\"income\"),\n               names_prefix= \"income_\",\n               names_to = \"income_bracket\",\n               values_to = \"percent\")%>%\n  mutate(hholds_bracket = round(hholds*(percent/100)),\n         income_bracket = recode(income_bracket,!!!income_brackets))\n\nushh_brackets"
  },
  {
    "objectID": "posts/challenge4_solutions.html",
    "href": "posts/challenge4_solutions.html",
    "title": "Challenge 4 Solutions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lubridate)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4_solutions.html#challenge-overview",
    "href": "posts/challenge4_solutions.html#challenge-overview",
    "title": "Challenge 4 Solutions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nidentify variables that need to be mutated\nmutate variables and sanity check all mutations\n\nTwo tidyverse packages will be used heavily today: lubridate (which is not automatically loaded) and stringr (which is part of core tidyverse).\n\nABC Poll ⭐Eggs ⭐⭐Fed Rates ⭐⭐⭐Hotel Bookings ⭐⭐⭐⭐Debt ⭐⭐⭐⭐⭐\n\n\n\nabc_poll_orig<-read_csv(\"_data/abc_poll_2021.csv\")\n\n# political questions\nabc_poll_orig%>%\n  select(starts_with(\"Q\"))%>%\n  colnames(.)\n\n [1] \"Q1_a\" \"Q1_b\" \"Q1_c\" \"Q1_d\" \"Q1_e\" \"Q1_f\" \"Q2\"   \"Q3\"   \"Q4\"   \"Q5\"  \n[11] \"QPID\"\n\n# all but one demographer\nabc_poll_orig%>%\n  select(starts_with(\"pp\"))%>%\n  colnames(.)\n\n [1] \"ppage\"    \"ppeduc5\"  \"ppeducat\" \"ppgender\" \"ppethm\"   \"pphhsize\"\n [7] \"ppinc7\"   \"ppmarit5\" \"ppmsacat\" \"ppreg4\"   \"pprent\"   \"ppstaten\"\n[13] \"PPWORKA\"  \"ppemploy\"\n\n# national poll\nn_distinct(abc_poll_orig$ppstaten)\n\n[1] 49\n\n\nThe ABC Poll appears to be a national sample survey (presumably from 2019) with 527 respondents. There are 10 political attitudes questions, plus party identification, in addition to 15 demographic variables (some with re-coded information) and 5 survey administration variables.\n\nprint(summarytools::dfSummary(abc_poll_orig,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\nData Frame Summary\nabc_poll_orig\nDimensions: 527 x 31\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      id\n[numeric]\n      Mean (sd) : 7230264 (152.3)min ≤ med ≤ max:7230001 ≤ 7230264 ≤ 7230527IQR (CV) : 263 (0)\n      527 distinct values\n      \n      0\n(0.0%)\n    \n    \n      xspanish\n[character]\n      1. English2. Spanish\n      514(97.5%)13(2.5%)\n      \n      0\n(0.0%)\n    \n    \n      complete_status\n[character]\n      1. qualified\n      527(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      ppage\n[numeric]\n      Mean (sd) : 53.4 (17.1)min ≤ med ≤ max:18 ≤ 55 ≤ 91IQR (CV) : 27 (0.3)\n      72 distinct values\n      \n      0\n(0.0%)\n    \n    \n      ppeduc5\n[character]\n      1. NA2. High school graduate (hig3. NA4. No high school diploma or5. Some college or Associate\n      108(20.5%)133(25.2%)99(18.8%)29(5.5%)158(30.0%)\n      \n      0\n(0.0%)\n    \n    \n      ppeducat\n[character]\n      1. Bachelors degree or highe2. High school3. Less than high school4. Some college\n      207(39.3%)133(25.2%)29(5.5%)158(30.0%)\n      \n      0\n(0.0%)\n    \n    \n      ppgender\n[character]\n      1. Female2. Male\n      254(48.2%)273(51.8%)\n      \n      0\n(0.0%)\n    \n    \n      ppethm\n[character]\n      1. 2+ Races, Non-Hispanic2. Black, Non-Hispanic3. Hispanic4. Other, Non-Hispanic5. White, Non-Hispanic\n      21(4.0%)27(5.1%)51(9.7%)24(4.6%)404(76.7%)\n      \n      0\n(0.0%)\n    \n    \n      pphhsize\n[character]\n      1. 12. 23. 34. 45. 56. 6 or more\n      80(15.2%)219(41.6%)102(19.4%)76(14.4%)35(6.6%)15(2.8%)\n      \n      0\n(0.0%)\n    \n    \n      ppinc7\n[character]\n      1. $10,000 to $24,9992. $100,000 to $149,9993. $150,000 or more4. $25,000 to $49,9995. $50,000 to $74,9996. $75,000 to $99,9997. Less than $10,000\n      32(6.1%)105(19.9%)137(26.0%)82(15.6%)85(16.1%)69(13.1%)17(3.2%)\n      \n      0\n(0.0%)\n    \n    \n      ppmarit5\n[character]\n      1. Divorced2. Never married3. Now Married4. Separated5. Widowed\n      43(8.2%)111(21.1%)337(63.9%)8(1.5%)28(5.3%)\n      \n      0\n(0.0%)\n    \n    \n      ppmsacat\n[character]\n      1. Metro area2. Non-metro area\n      448(85.0%)79(15.0%)\n      \n      0\n(0.0%)\n    \n    \n      ppreg4\n[character]\n      1. MidWest2. NorthEast3. South4. West\n      118(22.4%)93(17.6%)190(36.1%)126(23.9%)\n      \n      0\n(0.0%)\n    \n    \n      pprent\n[character]\n      1. Occupied without payment 2. Owned or being bought by 3. Rented for cash\n      10(1.9%)406(77.0%)111(21.1%)\n      \n      0\n(0.0%)\n    \n    \n      ppstaten\n[character]\n      1. California2. Texas3. Florida4. Pennsylvania5. Illinois6. New Jersey7. Ohio8. Michigan9. New York10. Washington[ 39 others ]\n      51(9.7%)42(8.0%)34(6.5%)28(5.3%)23(4.4%)21(4.0%)21(4.0%)18(3.4%)18(3.4%)18(3.4%)253(48.0%)\n      \n      0\n(0.0%)\n    \n    \n      PPWORKA\n[character]\n      1. Currently laid off2. Employed full-time (by so3. Employed part-time (by so4. Full Time Student5. Homemaker6. On furlough7. Other8. Retired9. Self-employed\n      13(2.5%)220(41.7%)31(5.9%)8(1.5%)37(7.0%)1(0.2%)20(3.8%)165(31.3%)32(6.1%)\n      \n      0\n(0.0%)\n    \n    \n      ppemploy\n[character]\n      1. Not working2. Working full-time3. Working part-time\n      221(41.9%)245(46.5%)61(11.6%)\n      \n      0\n(0.0%)\n    \n    \n      Q1_a\n[character]\n      1. Approve2. Disapprove3. Skipped\n      329(62.4%)193(36.6%)5(0.9%)\n      \n      0\n(0.0%)\n    \n    \n      Q1_b\n[character]\n      1. Approve2. Disapprove3. Skipped\n      192(36.4%)322(61.1%)13(2.5%)\n      \n      0\n(0.0%)\n    \n    \n      Q1_c\n[character]\n      1. Approve2. Disapprove3. Skipped\n      272(51.6%)248(47.1%)7(1.3%)\n      \n      0\n(0.0%)\n    \n    \n      Q1_d\n[character]\n      1. Approve2. Disapprove3. Skipped\n      192(36.4%)321(60.9%)14(2.7%)\n      \n      0\n(0.0%)\n    \n    \n      Q1_e\n[character]\n      1. Approve2. Disapprove3. Skipped\n      212(40.2%)301(57.1%)14(2.7%)\n      \n      0\n(0.0%)\n    \n    \n      Q1_f\n[character]\n      1. Approve2. Disapprove3. Skipped\n      281(53.3%)230(43.6%)16(3.0%)\n      \n      0\n(0.0%)\n    \n    \n      Q2\n[character]\n      1. Not concerned at all2. Not so concerned3. Somewhat concerned4. Very concerned\n      65(12.3%)147(27.9%)221(41.9%)94(17.8%)\n      \n      0\n(0.0%)\n    \n    \n      Q3\n[character]\n      1. No2. Skipped3. Yes\n      107(20.3%)5(0.9%)415(78.7%)\n      \n      0\n(0.0%)\n    \n    \n      Q4\n[character]\n      1. Excellent2. Good3. Not so good4. Poor5. Skipped\n      60(11.4%)215(40.8%)97(18.4%)149(28.3%)6(1.1%)\n      \n      0\n(0.0%)\n    \n    \n      Q5\n[character]\n      1. Optimistic2. Pessimistic3. Skipped\n      229(43.5%)295(56.0%)3(0.6%)\n      \n      0\n(0.0%)\n    \n    \n      QPID\n[character]\n      1. A Democrat2. A Republican3. An Independent4. Skipped5. Something else\n      176(33.4%)152(28.8%)168(31.9%)3(0.6%)28(5.3%)\n      \n      0\n(0.0%)\n    \n    \n      ABCAGE\n[character]\n      1. 18-292. 30-493. 50-644. 65+\n      60(11.4%)148(28.1%)157(29.8%)162(30.7%)\n      \n      0\n(0.0%)\n    \n    \n      Contact\n[character]\n      1. No, I am not willing to b2. Yes, I am willing to be i\n      355(67.4%)172(32.6%)\n      \n      0\n(0.0%)\n    \n    \n      weights_pid\n[numeric]\n      Mean (sd) : 1 (0.6)min ≤ med ≤ max:0.3 ≤ 0.8 ≤ 6.3IQR (CV) : 0.5 (0.6)\n      453 distinct values\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-08-23\n\n\n\n\nMutate PartyID\n\n\nThere are lots of string variables that might need to be modified for analysis or visualization. For example, the party id variable has “A Democrat” not the more standard language. Plus, there is a response “skipped” that should be treated as missing data. Lets see if we can fix it.\n\n#starting point\ntable(abc_poll_orig$QPID)\n\n\n    A Democrat   A Republican An Independent        Skipped Something else \n           176            152            168              3             28 \n\n#mutate\nabc_poll<-abc_poll_orig%>%\n  mutate(partyid = str_remove(QPID, \"A[n]* \"),\n         partyid = na_if(partyid, \"Skipped\"))%>%\n  select(-QPID)\n\n#sanity check\ntable(abc_poll$partyid)\n\n\n      Democrat    Independent     Republican Something else \n           176            168            152             28 \n\n\n\nEthnic Identity\nThe ethnic identity variable is long and could be tough to include in graphs, lets see if we can modify it - but we would need to include a table note to explain what the data labels mean (e.g., that racial labels mean non-hispanic, and that hispanic responses don’t indicate race.)\n\n#starting point\ntable(abc_poll$ppethm)\n\n\n2+ Races, Non-Hispanic    Black, Non-Hispanic               Hispanic \n                    21                     27                     51 \n   Other, Non-Hispanic    White, Non-Hispanic \n                    24                    404 \n\n#mutate\nabc_poll<-abc_poll%>%\n  mutate(ethnic = str_remove(ppethm, \", Non-Hispanic\"))%>%\n  select(-ppethm)\n\n#sanity check\ntable(abc_poll$ethnic)\n\n\n2+ Races    Black Hispanic    Other    White \n      21       27       51       24      404 \n\n\n\n\nRemoving “Skipped”\nWhat about the political variables that all have “Skipped” - a value that should probably be replaced with NA for analysis. Lets use the across function to make this easier.\n\nabc_poll<-abc_poll%>%\n  mutate(across(starts_with(\"Q\"), ~ na_if(.x, \"Skipped\")))\n\nmap(select(abc_poll, starts_with(\"Q1\")), table)\n\n$Q1_a\n\n   Approve Disapprove \n       329        193 \n\n$Q1_b\n\n   Approve Disapprove \n       192        322 \n\n$Q1_c\n\n   Approve Disapprove \n       272        248 \n\n$Q1_d\n\n   Approve Disapprove \n       192        321 \n\n$Q1_e\n\n   Approve Disapprove \n       212        301 \n\n$Q1_f\n\n   Approve Disapprove \n       281        230 \n\n\n\n\nFactor order\nFinally, what if you would like the categories of your variable to appear in a specific order, like the education variable that is currently in alphabetical order?\n\n\n\n\n\n\nfactor()\n\n\n\nThe factor variable type links variable labels to an underlying numeric order, and allows you to maintain the specified order for tables and graphics. Character strings always appear in alphabetical order.\n\n\n\ntable(abc_poll$ppeducat)\n\n\nBachelors degree or higher                High school \n                       207                        133 \n     Less than high school               Some college \n                        29                        158 \n\nedulabs <- unique(abc_poll$ppeducat)\nedulabs\n\n[1] \"High school\"                \"Bachelors degree or higher\"\n[3] \"Some college\"               \"Less than high school\"     \n\nabc_poll<-abc_poll%>%\n  mutate(educ = factor(ppeducat, \n                       levels=edulabs[c(4,1,3,2)]))%>%\n  select(-ppeducat)\nrm(edulabs)\n\ntable(abc_poll$educ)\n\n\n     Less than high school                High school \n                        29                        133 \n              Some college Bachelors degree or higher \n                       158                        207 \n\n\n\n\n\n\n\n\nThis section builds on the code available in the solution to Challenge 3, where we pivoted the organic eggs pricing data. The data reports the average price per carton paid to the farmer or producer for organic eggs (and organic chicken), reported monthly from 2004 to 20013. Average price is reported by carton type, which can vary in both size (x-large or large) and quantity (half-dozen or dozen.)\n\nRead Data\nWe are reading in half of the data from this workbook - the other half contains information about the price of organic chicken.\n\neggs_orig<-read_excel(\"_data/organiceggpoultry.xls\",\n                      sheet=\"Data\",\n                      range =cell_limits(c(6,2),c(NA,6)),\n                      col_names = c(\"date\", \"xlarge_dozen\",\n                               \"xlarge_halfdozen\", \"large_dozen\",\n                               \"large_halfdozen\")\n                 )\n\n\n\nClean and Mutate\nWe are going to be removing the note from the first column of the data, and splitting the year and month, and pivoting into long format prior to transforming the year and month columns into a date.\n\neggs<-eggs_orig%>%\n  mutate(date = str_remove(date, \" /1\"))%>%\n  separate(date, into=c(\"month\", \"year\"), sep=\" \")%>%\n  fill(year) %>%\n  pivot_longer(cols=contains(\"large\"),\n               names_to = c(\"size\", \"quantity\"),\n               names_sep=\"_\",\n               values_to = \"price\")\n\nNow, we need to create a date from a month and year. I can see that the months are a mix of long month name and 3 character month (for January), and the years are four digit years. Do I need to adjust the string for month manually, or can lubridate fix things for me?\nI’m going to combine the month with the now complete year column, and the parse the “month-year” format using my().\n\neggs<-eggs%>%\n  mutate(date = str_c(month, year, sep=\" \"),\n         date = my(date))\n\nselect(eggs, month, year, date)\n\n\n\n  \n\n\n\nInteresting - lubridate automatically fills in the first day of the month. Maybe we would prefer the last day, or even the middle of the month?\nNote that we can’t easily use make_datetime() for this example, as we would then need to transform the irregular month names into numeric values.\n\neggs<-eggs%>%\n  mutate(date = make_datetime(month, \"15\", year))\n\nselect(eggs, month, year, date)\n\n\n\n  \n\n\n\n\n\n\nThis data set runs from July 1954 to March 2017, and includes daily macroeconomic indicators related to the effective federal funds rate - or the interest rate at which banks lend money to each other in order to meet mandated reserve requirements.\nA single case is a year-month-day, and there are 7 values that can be pivoted or not depending on the needs of the analyst. 4 values are related to the federal funds rate: target, upper target, lower target, and effective), while 3 are related macroeconomic indicators (inflation, \\(\\bigtriangleup\\) GDP, and unemployment rate.)\nFor now, lets just focus on mutating the date.\n\nfed_rates_orig<-read_csv(\"_data/FedFundsRate.csv\")\n\nfed_rates_orig\n\n\n\n  \n\n\n\nOnce again, it looks like we will need to combine the year, month and date using stringr::str_c(), then we can use lubridate to transform into a date. Alternatively, because both month and day are numeric variables, we can use make_datetime().\n\nfed_rates<-fed_rates_orig%>%\n  mutate(date = str_c(Year, Month, Day, sep=\"-\"),\n         date = ymd(date))\n\nsummary(fed_rates$date)\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"1954-07-01\" \"1973-04-23\" \"1987-12-16\" \"1987-02-25\" \"2001-06-07\" \"2017-03-16\" \n\n\n\n\n\n\n\n\nGoing Further\n\n\n\nYou can now go through and figure out whether there are patterns in the missing-ness of specific indicators by date (maybe the values are only measured once a month or once a quarter, and we need to use fill(), or maybe there is something else going on?)\n\n\n\n\nThis data set contains 119,390 hotel bookings from two hotels (“City Hotel” and “Resort Hotel”) with an arrival date between July 2015 and August 2017 (more detail needed), including bookings that were later cancelled. See Solution Set 2 for additional details. The data are a de-identified extract of real hotel demand data, made available by the authors.\n\nbookings_orig<-read_csv(\"_data/hotel_bookings.csv\")\n\nselect(bookings_orig, starts_with(\"arrival\"))\n\n\n\n  \n\n\n\nLast time we looked at these data, I went to pretty extraordinary lengths to confirm the dates covered by the data. Lets see how much easier that is if we set the date to a date type variable instead! Those are long variable names, thank goodness we can get rid of them. Note that we only need three pieces of information out of the four provided.\nLook how I can mess around with the format, and lubridate still recovers the date!\n\nbookings<-bookings_orig%>%\n  mutate(date_arrival = str_c(arrival_date_day_of_month,\n                              arrival_date_month,\n                              arrival_date_year, sep=\"/\"),\n         date_arrival = dmy(date_arrival))%>%\n  select(-starts_with(\"arrival\"))\n\nsummary(bookings$date_arrival)\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"2015-07-01\" \"2016-03-13\" \"2016-09-06\" \"2016-08-28\" \"2017-03-18\" \"2017-08-31\" \n\n\nThere are other relevant time variables in the data set that may be worth exploring. For example, we are given a lead time measure in days (integer), but we could recover a date with lubridate. This would allow us to more easily visually explore, for example, if some people were more likely to make bookings over the winter for summer trips, but in fall for winter trips - or some other seasonal pattern.\n\nbookings<-bookings%>%\n  mutate(date_booking = date_arrival-days(lead_time))\n\nsummary(bookings$date_booking)\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"2013-06-24\" \"2015-11-28\" \"2016-05-04\" \"2016-05-16\" \"2016-12-09\" \"2017-08-31\" \n\n\nWe can also go in the reverse order. So if we wanted to know how many days before a booking there was last a change in the reservation status, we can generate this by comparing arrival date to reservation status date.\n\nsummary(bookings$reservation_status_date)\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"2014-10-17\" \"2016-02-01\" \"2016-08-07\" \"2016-07-30\" \"2017-02-08\" \"2017-09-14\" \n\nbookings<-bookings%>%\n  mutate(change_days = interval(reservation_status_date,\n                                date_arrival),\n         change_days = change_days %/% days(1))\n\nsummary(bookings$change_days)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -69.00   -3.00   -1.00   29.68   26.00  526.00 \n\n\n\n\nThis data set runs from the first quarter of 2003 to the second quarter of 2021, and includes quarterly measures of the total amount of household debt associated with 6 different types of loans - mortgage,HE revolving, auto, credit card, student, and other - plus a total household debt including all 6 loan types. This is another fantastic macroeconomic data product from the New York Federal Reserve. Detailed notes on the website reveal that the data are from an Equifax, and explain why data prior to 2003 is no longer part of the primary data publication.\n\ndebt_orig<-read_excel(\"_data/debt_in_trillions.xlsx\")\n\ndebt_orig\n\n\n\n  \n\n\n\nA single case is a year-quarter, and there are 6 (or 7) values that can be pivoted or not depending on the needs of the analyst. The tricky part is figuring out how to tell R to treat the quarters as a date! We could take the long road and separate the year and quarter information, then fix the year to be numeric, recombine, etc. But lets use the more complex formats option of parse_date plus a little regular expression style knowledge and read the information directly.\n\ndebt<-debt_orig%>%\n  mutate(date = parse_date_time(`Year and Quarter`, \n                           orders=\"yq\"))\n\nsummary(debt$date)\n\n                      Min.                    1st Qu. \n\"2003-01-01 00:00:00.0000\" \"2007-07-24 00:00:00.0000\" \n                    Median                       Mean \n\"2012-02-15 12:00:00.0000\" \"2012-02-15 06:09:43.7837\" \n                   3rd Qu.                       Max. \n\"2016-09-08 00:00:00.0000\" \"2021-04-01 00:00:00.0000\" \n\n\nWow, isn’t that super simple!"
  },
  {
    "objectID": "posts/challenge4_solutions.html#removing-skipped",
    "href": "posts/challenge4_solutions.html#removing-skipped",
    "title": "Challenge 4 Solutions",
    "section": "Removing “Skipped”",
    "text": "Removing “Skipped”\nWhat about the political variables that all have “Skipped” - a value that should probably be replaced with NA for analysis. Lets use the across function to make this easier.\n\nabc_poll<-abc_poll%>%\n  mutate(across(starts_with(\"Q\"), ~ na_if(.x, \"Skipped\")))\n\nmap(select(abc_poll, starts_with(\"Q1\")), table)\n\n$Q1_a\n\n   Approve Disapprove \n       329        193 \n\n$Q1_b\n\n   Approve Disapprove \n       192        322 \n\n$Q1_c\n\n   Approve Disapprove \n       272        248 \n\n$Q1_d\n\n   Approve Disapprove \n       192        321 \n\n$Q1_e\n\n   Approve Disapprove \n       212        301 \n\n$Q1_f\n\n   Approve Disapprove \n       281        230"
  },
  {
    "objectID": "posts/challenge4_solutions.html#factor-order",
    "href": "posts/challenge4_solutions.html#factor-order",
    "title": "Challenge 4 Solutions",
    "section": "Factor order",
    "text": "Factor order\nFinally, what if you would like the categories of your variable to appear in a specific order, like the education variable that is currently in alphabetical order?\n\n\n\n\n\n\nfactor()\n\n\n\nThe factor variable type links variable labels to an underlying numeric order, and allows you to maintain the specified order for tables and graphics. Character strings always appear in alphabetical order.\n\n\n\ntable(abc_poll$ppeducat)\n\n\nBachelors degree or higher                High school \n                       207                        133 \n     Less than high school               Some college \n                        29                        158 \n\nedulabs <- unique(abc_poll$ppeducat)\nedulabs\n\n[1] \"High school\"                \"Bachelors degree or higher\"\n[3] \"Some college\"               \"Less than high school\"     \n\nabc_poll<-abc_poll%>%\n  mutate(educ = factor(ppeducat, \n                       levels=edulabs[c(4,1,3,2)]))%>%\n  select(-ppeducat)\nrm(edulabs)\n\ntable(abc_poll$educ)\n\n\n     Less than high school                High school \n                        29                        133 \n              Some college Bachelors degree or higher \n                       158                        207"
  },
  {
    "objectID": "posts/challenge4_instructions.html",
    "href": "posts/challenge4_instructions.html",
    "title": "Challenge 4 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4_instructions.html#challenge-overview",
    "href": "posts/challenge4_instructions.html#challenge-overview",
    "title": "Challenge 4 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nidentify variables that need to be mutated\nmutate variables and sanity check all mutations"
  },
  {
    "objectID": "posts/challenge4_instructions.html#read-in-data",
    "href": "posts/challenge4_instructions.html#read-in-data",
    "title": "Challenge 4 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nabc_poll.csv ⭐\npoultry_tidy.csv⭐⭐\nFedFundsRate.csv⭐⭐⭐\nhotel_bookings.csv⭐⭐⭐⭐\ndebt_in_trillions ⭐⭐⭐⭐⭐\n\n\n\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge4_instructions.html#tidy-data-as-needed",
    "href": "posts/challenge4_instructions.html#tidy-data-as-needed",
    "title": "Challenge 4 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\n\n\n\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge4_instructions.html#identify-variables-that-need-to-be-mutated",
    "href": "posts/challenge4_instructions.html#identify-variables-that-need-to-be-mutated",
    "title": "Challenge 4 Instructions",
    "section": "Identify variables that need to be mutated",
    "text": "Identify variables that need to be mutated\nAre there any variables that require mutation to be usable in your analysis stream? For example, are all time variables correctly coded as dates? Are all string variables reduced and cleaned to sensible categories? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here.\n\n\n\nAny additional comments?"
  },
  {
    "objectID": "about/ProfRolfe.html",
    "href": "about/ProfRolfe.html",
    "title": "Meredith Rolfe",
    "section": "",
    "text": "Associate Professor, Political Science | UMass Amherst Lecturer in Public Management | London School of Economics Nuffield Posdoctoral Prize Fellow | Oxford University\nPhD, Political Science | University of Chicago BA, Comparative Area Studies | Duke University"
  },
  {
    "objectID": "about/ProfRolfe.html#r-experience",
    "href": "about/ProfRolfe.html#r-experience",
    "title": "Meredith Rolfe",
    "section": "R experience",
    "text": "R experience\nI was using R before it was cool (and when it was still called S…)"
  },
  {
    "objectID": "about/ProfRolfe.html#research-interests",
    "href": "about/ProfRolfe.html#research-interests",
    "title": "Meredith Rolfe",
    "section": "Research interests",
    "text": "Research interests\nTheories that combine social interaction and cognitive micro-foundations; innovative applications of methods of any sort (networks, text, simulations, experiments, surveys…)"
  },
  {
    "objectID": "about/ProfRolfe.html#hometown",
    "href": "about/ProfRolfe.html#hometown",
    "title": "Meredith Rolfe",
    "section": "Hometown",
    "text": "Hometown\nCharlotte, NC"
  },
  {
    "objectID": "about/ProfRolfe.html#hobbies",
    "href": "about/ProfRolfe.html#hobbies",
    "title": "Meredith Rolfe",
    "section": "Hobbies",
    "text": "Hobbies\num, DACSS???"
  },
  {
    "objectID": "about/ProfRolfe.html#fun-fact",
    "href": "about/ProfRolfe.html#fun-fact",
    "title": "Meredith Rolfe",
    "section": "Fun fact",
    "text": "Fun fact\ncoffee is my favorite food"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DACSS 601 August 2022",
    "section": "",
    "text": "Challenge 8 Instructions\n\n\n\n\n\n\n\nchallenge_8\n\n\nrailroads\n\n\nsnl\n\n\nfaostat\n\n\ndebt\n\n\n\n\nJoining Data\n\n\n\n\n\n\nAug 25, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 7 Instructions\n\n\n\n\n\n\n\nchallenge_7\n\n\nhotel_bookings\n\n\naustralian_marriage\n\n\nair_bnb\n\n\neggs\n\n\nabc_poll\n\n\nfaostat\n\n\nus_hh\n\n\n\n\nVisualizing Multiple Dimensions\n\n\n\n\n\n\nAug 24, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n  \n\n\n\n\nChallenge 8 Solutions\n\n\n\n\n\n\n\nchallenge_8\n\n\nactiveduty\n\n\nsnl\n\n\nfaostat\n\n\n\n\n\n\n\n\n\n\n\nAug 24, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 5 Solutions\n\n\n\n\n\n\n\nchallenge_5\n\n\nrailroads\n\n\ncereal\n\n\nair_bnb\n\n\npathogen_cost\n\n\naustralian_marriage\n\n\npublic_schools\n\n\nusa_hh\n\n\n\n\nIntroduction to Visualization\n\n\n\n\n\n\nAug 23, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 6 Solutions\n\n\n\n\n\n\n\nchallenge_6\n\n\nhotel_bookings\n\n\nair_bnb\n\n\nfed_rate\n\n\ndebt\n\n\nusa_hh\n\n\nabc_poll\n\n\n\n\nVisualizing Time and Relationships\n\n\n\n\n\n\nAug 23, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 6 Instructions\n\n\n\n\n\n\n\nchallenge_6\n\n\nhotel_bookings\n\n\nair_bnb\n\n\nfed_rate\n\n\ndebt\n\n\nusa_hh\n\n\nabc_poll\n\n\n\n\nVisualizing Time and Relationships\n\n\n\n\n\n\nAug 23, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 5 Instructions\n\n\n\n\n\n\n\nchallenge_5\n\n\nrailroads\n\n\ncereal\n\n\nair_bnb\n\n\npathogen_cost\n\n\naustralian_marriage\n\n\npublic_schools\n\n\nusa_hh\n\n\n\n\nIntroduction to Visualization\n\n\n\n\n\n\nAug 22, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4 Solutions\n\n\n\n\n\n\n\nchallenge_4\n\n\n\n\nMore data wrangling: mutate\n\n\n\n\n\n\nAug 21, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n  \n\n\n\n\nChallenge 3 Solutions\n\n\n\n\n\n\n\nchallenge_3\n\n\nsolution\n\n\n\n\nTidy Data: Pivoting\n\n\n\n\n\n\nAug 18, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4 Instructions\n\n\n\n\n\n\n\nchallenge_4\n\n\n\n\nMore data wrangling: mutate\n\n\n\n\n\n\nAug 18, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3 Instructions\n\n\n\n\n\n\n\nchallenge_3\n\n\n\n\nTidy Data: Pivoting\n\n\n\n\n\n\nAug 17, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 Solutions\n\n\n\n\n\n\n\nchallenge_2\n\n\nsolution\n\n\n\n\nData wrangling: using group() and summarise()\n\n\n\n\n\n\nAug 17, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n  \n\n\n\n\nChallenge 1 Solution\n\n\n\n\n\n\n\nchallenge_1\n\n\nsolution\n\n\n\n\nReading in data and creating a post\n\n\n\n\n\n\nAug 16, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 Instructions\n\n\n\n\n\n\n\nchallenge_2\n\n\n\n\nData wrangling: using group() and summarise()\n\n\n\n\n\n\nAug 16, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1 Instructions\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\nReading in data and creating a post\n\n\n\n\n\n\nAug 15, 2022\n\n\nMeredith Rolfe\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Contributors",
    "section": "",
    "text": "Meredith Rolfe\n\n\n\n\n\n\n\nNo matching items"
  }
]